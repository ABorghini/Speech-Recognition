{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPCtXV9cYZ0e"
      },
      "source": [
        "# Conformer implemantation\n",
        "Code inspiered by: https://github.com/sooftware/conformer and https://github.com/burchim/EfficientConformer\n",
        "\n",
        "Author: Borghini Alessia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTl1vb3vYojb"
      },
      "source": [
        "Check the notebook GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4ECvAlYEsKf",
        "outputId": "99c9bf93-5f67-4cb4-d5b1-c3de647e4840"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBajfdoFkB7p",
        "outputId": "f3564ac5-c489-4824-82fd-5ea32c31adaf"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive \n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9M0AHU1kJ04"
      },
      "source": [
        "# Import prerequirements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPKklu4NYlhF"
      },
      "source": [
        "Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOOv3u_RSghU"
      },
      "outputs": [],
      "source": [
        "!pip install -r drive/My\\ Drive/Speech_Recognition/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5ke-65IYunj"
      },
      "source": [
        "Import libraries and set seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAfXnCSvjyto"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch \n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader, Sampler\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchaudio\n",
        "\n",
        "import numpy as np\n",
        "import librosa\n",
        "from tqdm import tqdm\n",
        "from typing import Tuple, Optional\n",
        "import math\n",
        "\n",
        "import jiwer\n",
        "import sentencepiece\n",
        "\n",
        "import warp_rnnt\n",
        "\n",
        "import transformers\n",
        "\n",
        "import os\n",
        "import csv\n",
        "\n",
        "PATH = \"drive/MyDrive/Speech_Recognition/\"\n",
        "\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsriWRlC0tZ1"
      },
      "source": [
        "# Librispeech Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WXnIYd_qKOf"
      },
      "outputs": [],
      "source": [
        "os.mkdir(\"datasets\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPD-_m5pX2gO",
        "outputId": "0685fa37-0c05-496c-c1dc-9b14aa9f8538"
      },
      "outputs": [],
      "source": [
        "# Download LibriSPeech train-clean-100 subset\n",
        "!cd datasets && wget https://www.openslr.org/resources/12/train-clean-100.tar.gz && tar xzf train-clean-100.tar.gz\n",
        "\n",
        "# Download LibriSPeech train-clean-360 subset\n",
        "# !cd datasets && wget https://www.openslr.org/resources/12/train-clean-360.tar.gz && tar xzf train-clean-360.tar.gz\n",
        "\n",
        "# Download LibriSPeech dev-clean subset\n",
        "!cd datasets && wget https://www.openslr.org/resources/12/dev-clean.tar.gz && tar xzf dev-clean.tar.gz\n",
        "\n",
        "# Download LibriSPeech dev-other subset\n",
        "# !cd datasets && wget https://www.openslr.org/resources/12/dev-other.tar.gz && tar xzf dev-other.tar.gz\n",
        "\n",
        "# Download LibriSPeech test-clean subset\n",
        "# !cd datasets && wget https://www.openslr.org/resources/12/test-clean.tar.gz && tar xzf test-clean.tar.gz\n",
        "\n",
        "# Download LibriSPeech test-other subset\n",
        "# !cd datasets && wget https://www.openslr.org/resources/12/test-other.tar.gz && tar xzf test-other.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQujR2RTmz1L"
      },
      "outputs": [],
      "source": [
        "import glob \n",
        "\n",
        "class LibriSpeechDataset():\n",
        "    def __init__(self, split, tokenizer):\n",
        "\n",
        "        print(\"Creating dataset..\")\n",
        "        self.data_names = glob.glob(\"datasets/LibriSpeech/\" + split + \"*/*/*/*.flac\")\n",
        "        self.vocab_type = \"bpe\"\n",
        "        self.vocab_size = 256\n",
        "\n",
        "        label_paths = []\n",
        "        sentences = []\n",
        "\n",
        "        for file_path in glob.glob(\"datasets/LibriSpeech/\" + split + \"/*/*/*.txt\"):\n",
        "            for line in open(file_path, \"r\").readlines():\n",
        "                label_paths.append(file_path.replace(file_path.split(\"/\")[-1], \"\") + line.split()[0] + \".\" + self.vocab_type + \"_\" + str(self.vocab_size))\n",
        "                sentences.append(line[len(line.split()[0]) + 1:-1].lower())\n",
        "\n",
        "        for (sentence, label_path) in tqdm(zip(sentences, label_paths)):\n",
        "            # Tokenize and Save label\n",
        "            label = torch.LongTensor(tokenizer.encode(sentence))\n",
        "            torch.save(label, label_path)\n",
        "\n",
        "            # Save Audio length\n",
        "            audio_length = torchaudio.load(label_path.split(\".\")[0] + \".flac\")[0].size(1)\n",
        "            torch.save(audio_length, label_path.split(\".\")[0] + \".flac_len\")\n",
        "\n",
        "            # Save Label length\n",
        "            label_length = label.size(0)\n",
        "            torch.save(label_length, label_path + \"_len\")\n",
        "            \n",
        "        print(\"Done.\")\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return [torchaudio.load(self.data_names[i])[0], \n",
        "                torch.load(self.data_names[i].split(\".flac\")[0] + \".\" + self.vocab_type + \"_\" + str(self.vocab_size)),\n",
        "                torch.load(self.data_names[i] + \"_len\"),\n",
        "                torch.load(self.data_names[i].split(\".flac\")[0] + \".\" + self.vocab_type + \"_\" + str(self.vocab_size) + \"_len\")]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTuHcp1SztS9"
      },
      "outputs": [],
      "source": [
        "# corpus_path = \"datasets/LibriSpeech/train-clean-100_corpus.txt\"\n",
        "\n",
        "# # Create Corpus File\n",
        "# if not os.path.isfile(corpus_path):\n",
        "#     print(\"Create Corpus File\")\n",
        "#     corpus_file = open(corpus_path, \"w\")\n",
        "#     for file_path in tqdm(glob.glob(\"datasets/LibriSpeech/*/*/*/*.txt\")):\n",
        "#         for line in open(file_path, \"r\").readlines():\n",
        "#             corpus_file.write(line[len(line.split()[0]) + 1:-1].lower() + \"\\n\")\n",
        "\n",
        "# # Train Tokenizer\n",
        "# print(\"Training Tokenizer\")\n",
        "# sentencepiece.SentencePieceTrainer.train(input=corpus_path, \n",
        "#                                          model_prefix=\"LibriSpeech_bpe_1000\", \n",
        "#                                          vocab_size=1000, \n",
        "#                                          character_coverage=1.0, \n",
        "#                                          model_type=\"bpe\", \n",
        "#                                          bos_id=-1, \n",
        "#                                          eos_id=-1, \n",
        "#                                          unk_surface=\"\")\n",
        "# print(\"Training Done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zci0AHMTpDH-",
        "outputId": "3cb2487f-74d2-4765-bb2f-9e09b12f9bcc"
      },
      "outputs": [],
      "source": [
        "tokenizer_path = \"drive/My Drive/Speech_Recognition/LibriSpeech_bpe_256.model\"\n",
        "tokenizer = sentencepiece.SentencePieceProcessor(tokenizer_path)\n",
        "train_dataset = LibriSpeechDataset(\"train-clean-100\", tokenizer)\n",
        "dev_dataset = LibriSpeechDataset(\"dev-clean\", tokenizer)\n",
        "\n",
        "# train_dataset.__getitem__(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDxwsnOy0ks_"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ls-O9ku_SKyd"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    # Sorting sequences by lengths\n",
        "    sorted_batch = sorted(batch, key=lambda x: x[0].shape[1], reverse=True)\n",
        "\n",
        "    # Pad data sequences\n",
        "    data = [item[0].squeeze() for item in sorted_batch]\n",
        "    data_lengths = torch.tensor([len(d) for d in data],dtype=torch.long).cuda()\n",
        "    data = torch.nn.utils.rnn.pad_sequence(data, batch_first=True, padding_value=0).cuda()\n",
        "\n",
        "    # Pad labels\n",
        "    target = [item[1] for item in sorted_batch]\n",
        "    target_lengths = torch.tensor([t.size(0) for t in target],dtype=torch.long).cuda()\n",
        "    target = torch.nn.utils.rnn.pad_sequence(target, batch_first=True, padding_value=0).cuda()\n",
        "\n",
        "    return data, target, data_lengths, target_lengths\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                               batch_size=4,\n",
        "                                               shuffle=True,\n",
        "                                               collate_fn=collate_fn,\n",
        "                                               drop_last=True)\n",
        "\n",
        "dev_dataloader = torch.utils.data.DataLoader(dev_dataset,\n",
        "                                             batch_size=4,\n",
        "                                             shuffle=False,\n",
        "                                             collate_fn=collate_fn,\n",
        "                                             drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSFQfnSJY7KH"
      },
      "outputs": [],
      "source": [
        "os.mkdir(\"datasets\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O17_StTrXb6d"
      },
      "source": [
        "Downloading LibriSpeech dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QEEi79rY7KL",
        "outputId": "90c6c6dc-108d-4524-eda2-9a0f99c562ce"
      },
      "outputs": [],
      "source": [
        "# Download LibriSPeech train-clean-100 subset\n",
        "# !cd datasets && wget https://www.openslr.org/resources/12/train-clean-100.tar.gz && tar xzf train-clean-100.tar.gz\n",
        "\n",
        "# Download LibriSPeech train-clean-360 subset\n",
        "# !cd datasets && wget https://www.openslr.org/resources/12/train-clean-360.tar.gz && tar xzf train-clean-360.tar.gz\n",
        "\n",
        "# Download LibriSPeech dev-clean subset\n",
        "!cd datasets && wget https://www.openslr.org/resources/12/dev-clean.tar.gz && tar xzf dev-clean.tar.gz\n",
        "\n",
        "# Download LibriSPeech dev-other subset\n",
        "# !cd datasets && wget https://www.openslr.org/resources/12/dev-other.tar.gz && tar xzf dev-other.tar.gz\n",
        "\n",
        "# Download LibriSPeech test-clean subset\n",
        "# !cd datasets && wget https://www.openslr.org/resources/12/test-clean.tar.gz && tar xzf test-clean.tar.gz\n",
        "\n",
        "# Download LibriSPeech test-other subset\n",
        "# !cd datasets && wget https://www.openslr.org/resources/12/test-other.tar.gz && tar xzf test-other.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCykbYKpXRf7"
      },
      "source": [
        "LibriSpeech dataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUIb3m-eY7KM"
      },
      "outputs": [],
      "source": [
        "import glob \n",
        "\n",
        "class LibriSpeechDataset():\n",
        "    def __init__(self, split, tokenizer):\n",
        "\n",
        "        print(\"Creating dataset..\")\n",
        "        self.data_names = glob.glob(\"datasets/LibriSpeech/\" + split + \"*/*/*/*.flac\")\n",
        "        self.vocab_type = \"bpe\"\n",
        "        self.vocab_size = 1000\n",
        "\n",
        "        # if split == \"train-clean-100\":\n",
        "        label_paths = []\n",
        "        sentences = []\n",
        "\n",
        "        for file_path in glob.glob(\"datasets/LibriSpeech/\" + split + \"/*/*/*.txt\"):\n",
        "            for line in open(file_path, \"r\").readlines():\n",
        "                label_paths.append(file_path.replace(file_path.split(\"/\")[-1], \"\") + line.split()[0] + \".\" + self.vocab_type + \"_\" + str(self.vocab_size))\n",
        "                sentences.append(line[len(line.split()[0]) + 1:-1].lower())\n",
        "\n",
        "        for (sentence, label_path) in tqdm(zip(sentences, label_paths)):\n",
        "            # Tokenize and Save label\n",
        "            label = torch.LongTensor(tokenizer.encode(sentence))\n",
        "            torch.save(label, label_path)\n",
        "\n",
        "            # Save Audio length\n",
        "            audio_length = torchaudio.load(label_path.split(\".\")[0] + \".flac\")[0].size(1)\n",
        "            torch.save(audio_length, label_path.split(\".\")[0] + \".flac_len\")\n",
        "\n",
        "            # Save Label length\n",
        "            label_length = label.size(0)\n",
        "            torch.save(label_length, label_path + \"_len\")\n",
        "                \n",
        "        print(\"Done.\")\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return [torchaudio.load(self.data_names[i])[0], \n",
        "                torch.load(self.data_names[i].split(\".flac\")[0] + \".\" + self.vocab_type + \"_\" + str(self.vocab_size)),\n",
        "                torch.load(self.data_names[i] + \"_len\"),\n",
        "                torch.load(self.data_names[i].split(\".flac\")[0] + \".\" + self.vocab_type + \"_\" + str(self.vocab_size) + \"_len\")]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlMnTkTpXNOq"
      },
      "source": [
        "Training the tokenizer (uncomment to create a new tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWJm0gZ6Y7KN"
      },
      "outputs": [],
      "source": [
        "# corpus_path = \"datasets/LibriSpeech/train-clean-100_corpus.txt\"\n",
        "\n",
        "# # Create Corpus File\n",
        "# if not os.path.isfile(corpus_path):\n",
        "#     print(\"Create Corpus File\")\n",
        "#     corpus_file = open(corpus_path, \"w\")\n",
        "#     for file_path in glob.glob(\"datasets/LibriSpeech/*/*/*/*.txt\"):\n",
        "#         for line in open(file_path, \"r\").readlines():\n",
        "#             corpus_file.write(line[len(line.split()[0]) + 1:-1].lower() + \"\\n\")\n",
        "\n",
        "# # Train Tokenizer\n",
        "# print(\"Training Tokenizer\")\n",
        "# sentencepiece.SentencePieceTrainer.train(input=corpus_path, \n",
        "#                                          model_prefix=\"LibriSpeech_bpe_256\", \n",
        "#                                          vocab_size=256, \n",
        "#                                          character_coverage=1.0, \n",
        "#                                          model_type=\"bpe\", \n",
        "#                                          bos_id=-1, \n",
        "#                                          eos_id=-1, \n",
        "#                                          unk_surface=\"\")\n",
        "# print(\"Training Done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysQiLozZYHe6"
      },
      "source": [
        "Build dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZx-mk-zY7KO",
        "outputId": "a01d1496-1072-4b8a-c326-ba83fc8b5d24"
      },
      "outputs": [],
      "source": [
        "tokenizer_path = PATH + \"LibriSpeech_bpe_256.model\"\n",
        "tokenizer = sentencepiece.SentencePieceProcessor(tokenizer_path)\n",
        "\n",
        "# train_dataset = LibriSpeechDataset(\"train-clean-100\", tokenizer)\n",
        "dev_dataset = LibriSpeechDataset(\"dev-clean\", tokenizer)\n",
        "\n",
        "# train_dataset.__getitem__(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6AwV9z-Y7KO"
      },
      "source": [
        "Build dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGXclF3oY7KP"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    # Sorting sequences by lengths\n",
        "    sorted_batch = sorted(batch, key=lambda x: x[0].shape[1], reverse=True)\n",
        "\n",
        "    # Pad data sequences\n",
        "    data = [item[0].squeeze() for item in sorted_batch]\n",
        "    data_lengths = torch.tensor([len(d) for d in data],dtype=torch.long).cuda()\n",
        "    data = torch.nn.utils.rnn.pad_sequence(data, batch_first=True, padding_value=0).cuda()\n",
        "\n",
        "    # Pad labels\n",
        "    target = [item[1] for item in sorted_batch]\n",
        "    target_lengths = torch.tensor([t.size(0) for t in target],dtype=torch.long).cuda()\n",
        "    target = torch.nn.utils.rnn.pad_sequence(target, batch_first=True, padding_value=0).cuda()\n",
        "\n",
        "    return data, target, data_lengths, target_lengths\n",
        "\n",
        "# train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
        "#                                                batch_size=4,\n",
        "#                                                shuffle=True,\n",
        "#                                                collate_fn=collate_fn,\n",
        "#                                                drop_last=True)\n",
        "\n",
        "dev_dataloader = torch.utils.data.DataLoader(dev_dataset,\n",
        "                                             batch_size=4,\n",
        "                                             shuffle=False,\n",
        "                                             collate_fn=collate_fn,\n",
        "                                             drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJLnkL3COfPU"
      },
      "source": [
        "# Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8171lGBwOeeh"
      },
      "outputs": [],
      "source": [
        "class Trainer():\n",
        "    def __init__(self,\n",
        "                 model: nn.Module,\n",
        "                 optimizer,\n",
        "                 scheduler,\n",
        "                 scaler,\n",
        "                 path,\n",
        "                 save_best_model = True,\n",
        "                 logging = True,\n",
        "                 accumulated_steps = 16):\n",
        "      \n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "        self.scaler = scaler\n",
        "        self.accumulated_steps = accumulated_steps\n",
        "\n",
        "        self.best_wer = 2\n",
        "        self.path = path\n",
        "        self.save_best_model = save_best_model\n",
        "        self.logging = logging\n",
        "\n",
        "    def train(self, \n",
        "              train_dataset,\n",
        "              dev_dataset,\n",
        "              epochs:int=20):\n",
        "        \n",
        "        print(\"Training...\")\n",
        "\n",
        "        train_loss = 0.0\n",
        "        total_loss_train = []\n",
        "        total_loss_dev = []\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(\" Epoch {:03d}\".format(epoch + 1))\n",
        "\n",
        "            epoch_loss = 0.0\n",
        "\n",
        "            # train mode on\n",
        "            self.model.train()\n",
        "\n",
        "            for step, batch in enumerate(tqdm(train_dataset)):        \n",
        "                inputs, targets, input_len, target_len = batch\n",
        "\n",
        "                # Automatic Mixed Precision Casting (model prediction + loss computing)\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    pred, pred_len = self.model.forward(inputs, input_len, targets, target_len, training=True)\n",
        "                    loss_mini  = warp_rnnt.rnnt_loss(\n",
        "                                    log_probs=torch.nn.functional.log_softmax(pred, dim=-1),\n",
        "                                    labels=targets.int(),\n",
        "                                    frames_lengths=pred_len.int(),\n",
        "                                    labels_lengths=target_len.int(),\n",
        "                                    average_frames=False,\n",
        "                                    reduction='mean',\n",
        "                                    blank=0,\n",
        "                                    gather=True)\n",
        "                    # loss_mini = lossfn(pred, y.int(), pred_len.int(), y_len.int())\n",
        "                    loss = loss_mini / self.accumulated_steps\n",
        "\n",
        "                # Accumulate gradients\n",
        "                scaler.scale(loss).backward()\n",
        "\n",
        "                # Update Epoch Variables\n",
        "                epoch_loss += loss_mini.detach()\n",
        "\n",
        "                if step % self.accumulated_steps == 0:\n",
        "                    # Update Parameters, Zero Gradients and Update Learning Rate\n",
        "                    self.scaler.step(self.optimizer)\n",
        "                    self.scaler.update()\n",
        "                    self.optimizer.zero_grad()\n",
        "                    self.scheduler.step()\n",
        "\n",
        "                    # Step Print\n",
        "                    print(\"mean loss {:.4f} - batch loss: {:.4f} - learning rate: {:.6f}\".format(epoch_loss / (step + 1), loss_mini, optimizer.param_groups[0]['lr']))\n",
        "\n",
        "            avg_epoch_loss = epoch_loss / len(dev_dataset)\n",
        "            print('\\t[E: {:2d}] train loss = {:0.4f}'.format(epoch, avg_epoch_loss))\n",
        "\n",
        "            wer, speech_true, speech_pred, val_loss = self.evaluate(dev_dataset)\n",
        "            print('\\t[E: {:2d}] valid loss = {:0.4f}, wer = {:0.4f}, loss = {:0.4f}'.format(epoch, val_loss, wer, avg_epoch_loss))\n",
        "\n",
        "            if self.save_best_model:\n",
        "                self.save_model(wer, avg_epoch_loss, optimizer.param_groups[0]['lr'], self.path)\n",
        "\n",
        "        print(\"...Done!\")\n",
        "        return avg_epoch_loss\n",
        "\n",
        "    def evaluate(self,\n",
        "              dev_dataset):\n",
        "        \n",
        "        self.model.eval()\n",
        "\n",
        "        speech_true = []\n",
        "        speech_pred = []\n",
        "        total_wer = 0.0\n",
        "        total_loss = 0.0\n",
        "\n",
        "        # Evaluation Loop\n",
        "        for step, batch in enumerate(tqdm(dev_dataset)):\n",
        "\n",
        "            inputs, targets, input_len, target_len = batch\n",
        "\n",
        "            # Sequence Prediction\n",
        "            with torch.no_grad():\n",
        "\n",
        "                outputs_pred = greedy_search_decoding(inputs, input_len)\n",
        "\n",
        "                # out = self.model.recognize(inputs, input_len)\n",
        "                # outputs_pred = tokenizer.decode(out.tolist())\n",
        "\n",
        "            # Sequence Truth\n",
        "            outputs_true = tokenizer.decode(targets.tolist())\n",
        "\n",
        "            # Compute Batch wer and Update total wer\n",
        "            batch_wer = jiwer.wer(outputs_true, outputs_pred, standardize=True)\n",
        "            total_wer += batch_wer\n",
        "\n",
        "            # Update String lists\n",
        "            speech_true += outputs_true\n",
        "            speech_pred += outputs_pred\n",
        "\n",
        "            # Prediction Verbose\n",
        "            print(\"Groundtruths :\\n\", outputs_true)\n",
        "            print(\"Predictions :\\n\", outputs_pred)\n",
        "\n",
        "            # Eval Loss\n",
        "            with torch.no_grad():\n",
        "                pred, pred_len = self.model.forward(inputs, input_len, targets, target_len)\n",
        "                batch_loss = warp_rnnt.rnnt_loss(\n",
        "                                    log_probs=torch.nn.functional.log_softmax(pred, dim=-1),\n",
        "                                    labels=targets.int(),\n",
        "                                    frames_lengths=pred_len.int(),\n",
        "                                    labels_lengths=target_len.int(),\n",
        "                                    average_frames=False,\n",
        "                                    reduction='mean',\n",
        "                                    blank=0,\n",
        "                                    gather=True)\n",
        "                # batch_loss = self.loss(pred, targets, pred_len, target_len)\n",
        "                total_loss += batch_loss\n",
        "\n",
        "            # Step print\n",
        "            print(\"mean batch wer {:.2f}% - batch wer: {:.2f}% - mean loss {:.4f} - batch loss: {:.4f}\".format(100 * total_wer / (step + 1), 100 * batch_wer, total_loss / (step + 1), batch_loss))\n",
        "\n",
        "        # Compute wer\n",
        "        if total_wer / dev_dataset.__len__() > 1:\n",
        "            wer = 1\n",
        "        else:\n",
        "            wer = jiwer.wer(speech_true, speech_pred, standardize=True)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = total_loss / dev_dataset.__len__()\n",
        "\n",
        "        return wer, speech_true, speech_pred, loss\n",
        "\n",
        "    def save_model(self, wer, loss, lr, path):\n",
        "        if wer < self.best_wer:\n",
        "            torch.save(self.model.state_dict(), f\"{path}.pth\")\n",
        "            torch.save(self.scheduler.state_dict(), f\"{path}_scheduler.pth\")\n",
        "            torch.save(self.optimizer.state_dict(), f\"{path}_optimizer.pth\")\n",
        "\n",
        "        if self.logging:\n",
        "            with open(f\"{path}.tsv\", \"a\") as log:\n",
        "                log.write(f\"{lr}\\t{wer}\\t{loss}\\n\")\n",
        "                "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TlKhCIUZAGq"
      },
      "source": [
        "Greedy search decoding strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CIDyn0mSV2m"
      },
      "outputs": [],
      "source": [
        "def greedy_search_decoding(x, x_len):\n",
        "    # Predictions String List\n",
        "    preds = []\n",
        "\n",
        "    # Forward Encoder (B, Taud) -> (B, T, Denc)\n",
        "    f, f_len = model.encoder(x, x_len)\n",
        "\n",
        "    # Batch loop\n",
        "    for b in range(x.size(0)): # One sample at a time for now, not batch optimized\n",
        "        # Init y and hidden state\n",
        "        y = x.new_zeros(1, 1, dtype=torch.long)\n",
        "        hidden = None\n",
        "\n",
        "        enc_step = 0\n",
        "        consec_dec_step = 0\n",
        "\n",
        "        # Decoder loop\n",
        "        while enc_step < f_len[b]:\n",
        "\n",
        "            # Forward Decoder (1, 1) -> (1, 1, Ddec)\n",
        "            g, hidden = model.decoder(y[:, -1:], hidden_states=hidden)\n",
        "            \n",
        "            # Joint Network loop\n",
        "            while enc_step < f_len[b]:\n",
        "\n",
        "                # Forward Joint Network (1, 1, Denc) and (1, 1, Ddec) -> (1, V)\n",
        "                logits = model.joint(f[b:b+1, enc_step], g[:, 0])\n",
        "\n",
        "                # Token Prediction\n",
        "                pred = logits.softmax(dim=-1).log().argmax(dim=-1) # (1)\n",
        "\n",
        "                # Null token or max_consec_dec_step\n",
        "                if pred == 0 or consec_dec_step == 5:\n",
        "                    consec_dec_step = 0\n",
        "                    enc_step += 1\n",
        "                # Token\n",
        "                else:\n",
        "                    consec_dec_step += 1\n",
        "                    y = torch.cat([y, pred.unsqueeze(0)], axis=-1)\n",
        "                    break\n",
        "\n",
        "        # Decode Label Sequence\n",
        "        pred = tokenizer.decode(y[:, 1:].tolist())\n",
        "        preds += pred\n",
        "\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSnaFiVaOmZF"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciykJgaBWIIB"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCCsv_kwfKHw"
      },
      "source": [
        "### Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dv0SbrHfWG9o"
      },
      "outputs": [],
      "source": [
        "class Linear(nn.Linear):\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias = True):\n",
        "        super(Linear, self).__init__(\n",
        "            in_features=in_features, \n",
        "            out_features=out_features, \n",
        "            bias=bias)\n",
        "\n",
        "        # Variational Noise\n",
        "        self.noise = None\n",
        "        self.vn_std = None\n",
        "\n",
        "    def init_vn(self, vn_std):\n",
        "\n",
        "        # Variational Noise\n",
        "        self.vn_std = vn_std\n",
        "\n",
        "    def sample_synaptic_noise(self, distributed):\n",
        "\n",
        "        # Sample Noise\n",
        "        self.noise = torch.normal(mean=0.0, std=1.0, size=self.weight.size(), device=self.weight.device, dtype=self.weight.dtype)\n",
        "\n",
        "        # Broadcast Noise\n",
        "        if distributed:\n",
        "            torch.distributed.broadcast(self.noise, 0)\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        # Weight\n",
        "        weight = self.weight\n",
        "\n",
        "        # Add Noise\n",
        "        if self.noise is not None and self.training:\n",
        "            weight = weight + self.vn_std * self.noise\n",
        "            \n",
        "        # Apply Weight\n",
        "        return F.linear(input, weight, self.bias)\n",
        "\n",
        "\n",
        "class View(nn.Module):\n",
        "    \"\"\" Wrapper class of torch.view() for Sequential module. \"\"\"\n",
        "    def __init__(self, shape: tuple, contiguous: bool = False):\n",
        "        super(View, self).__init__()\n",
        "        self.shape = shape\n",
        "        self.contiguous = contiguous\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        if self.contiguous:\n",
        "            x = x.contiguous()\n",
        "\n",
        "        return x.view(*self.shape)\n",
        "\n",
        "class Transpose(nn.Module):\n",
        "\n",
        "    def __init__(self, dim0, dim1):\n",
        "        super(Transpose, self).__init__()\n",
        "        self.dim0 = dim0\n",
        "        self.dim1 = dim1\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x.transpose(self.dim0, self.dim1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yo4XOSWSNeQj"
      },
      "outputs": [],
      "source": [
        "class Conv2dSubampling(nn.Module):\n",
        "    \"\"\"\n",
        "    Convolutional 2D subsampling (to 1/4 length)\n",
        "    Args:\n",
        "        in_channels (int): Number of channels in the input image\n",
        "        out_channels (int): Number of channels produced by the convolution\n",
        "    Inputs: inputs\n",
        "        - **inputs** (batch, time, dim): Tensor containing sequence of inputs\n",
        "    Returns: outputs, output_lengths\n",
        "        - **outputs** (batch, time, dim): Tensor produced by the convolution\n",
        "        - **output_lengths** (batch): list of sequence output lengths\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, num_conv_layers, kernel_size=3) -> None:\n",
        "        super(Conv2dSubampling, self).__init__()\n",
        "\n",
        "        self.sequential = nn.ModuleList([nn.Sequential(\n",
        "            nn.Conv2d(in_channels if id == 0 else out_channels, \n",
        "                      out_channels, \n",
        "                      kernel_size, \n",
        "                      stride=2, \n",
        "                      padding=(kernel_size - 1) // 2), \n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            Swish()\n",
        "        ) for id in range(num_conv_layers)])\n",
        "\n",
        "\n",
        "    def forward(self, inputs: Tensor, input_lengths: Tensor) -> Tuple[Tensor, Tensor]:\n",
        "        outputs = inputs.unsqueeze(1)\n",
        "\n",
        "        for layer in self.sequential:\n",
        "            outputs = layer(outputs)\n",
        "\n",
        "        output_lengths = input_lengths\n",
        "        batch_size, channels, subsampled_lengths, sumsampled_dim = outputs.size()\n",
        "\n",
        "        outputs = outputs.permute(0, 2, 1, 3)\n",
        "        outputs = outputs.contiguous().view(batch_size, subsampled_lengths, channels * sumsampled_dim)\n",
        "\n",
        "        output_lengths = torch.div(input_lengths - 1, 2, rounding_mode='floor') + 1\n",
        "        output_lengths = torch.div(output_lengths - 1, 2, rounding_mode='floor') + 1\n",
        "\n",
        "        return outputs, output_lengths\n",
        "\n",
        "\n",
        "class Conv1d(nn.Conv1d):\n",
        "\n",
        "    def __init__(\n",
        "        self, \n",
        "        in_channels, \n",
        "        out_channels, \n",
        "        kernel_size, \n",
        "        stride = 1, \n",
        "        padding = \"same\", \n",
        "        dilation = 1, \n",
        "        groups = 1, \n",
        "        bias = True\n",
        "    ):\n",
        "        super(Conv1d, self).__init__(\n",
        "            in_channels=in_channels, \n",
        "            out_channels=out_channels, \n",
        "            kernel_size=kernel_size, \n",
        "            stride=stride, \n",
        "            padding=0, \n",
        "            dilation=dilation, \n",
        "            groups=groups, \n",
        "            bias=bias, \n",
        "            padding_mode=\"zeros\")\n",
        "\n",
        "        # Assert\n",
        "        assert padding in [\"valid\", \"same\", \"causal\"]\n",
        "\n",
        "        # Padding\n",
        "        if padding == \"valid\":\n",
        "            self.pre_padding = None\n",
        "        elif padding == \"same\":\n",
        "            self.pre_padding = nn.ConstantPad1d(padding=((kernel_size - 1) // 2, (kernel_size - 1) // 2), value=0)\n",
        "        elif padding == \"causal\":\n",
        "            self.pre_padding = nn.ConstantPad1d(padding=(kernel_size - 1, 0), value=0)\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        # Weight\n",
        "        weight = self.weight\n",
        "\n",
        "        # Padding\n",
        "        if self.pre_padding is not None:\n",
        "            input = self.pre_padding(input)\n",
        "\n",
        "        # Apply Weight\n",
        "        return F.conv1d(input, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8TFEHrKZ4aA"
      },
      "source": [
        "### Feed Forward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTTuJOC5Z7PY"
      },
      "outputs": [],
      "source": [
        "class FeedForwardModule(nn.Module):\n",
        "\n",
        "    \"\"\"Transformer Feed Forward Module\n",
        "    Args:\n",
        "        dim_model: model feature dimension\n",
        "        dim_ffn: expanded feature dimension\n",
        "        Pdrop: dropout probability\n",
        "        act: inner activation function\n",
        "        inner_dropout: whether to apply dropout after the inner activation function\n",
        "    Input: (batch size, length, dim_model)\n",
        "    Output: (batch size, length, dim_model)\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim_model, dim_ffn, Pdrop, act, inner_dropout):\n",
        "        super(FeedForwardModule, self).__init__()\n",
        "\n",
        "        # Assert\n",
        "        assert act in [\"relu\", \"swish\"]\n",
        "\n",
        "        # Layers\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.LayerNorm(dim_model, eps=1e-6),\n",
        "            Linear(dim_model, dim_ffn),\n",
        "            Swish() if act==\"swish\" else nn.ReLU(),\n",
        "            nn.Dropout(p=Pdrop) if inner_dropout else nn.Identity(),\n",
        "            Linear(dim_ffn, dim_model),\n",
        "            nn.Dropout(p=Pdrop)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4knMA-P4ZBsj"
      },
      "source": [
        "### Activation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaDwidJfZDqp"
      },
      "outputs": [],
      "source": [
        "class Swish(nn.Module):\n",
        "    \"\"\"\n",
        "    Swish is a smooth, non-monotonic function that consistently matches or outperforms ReLU on deep networks applied\n",
        "    to a variety of challenging domains such as Image classification and Machine translation.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(Swish, self).__init__()\n",
        "    \n",
        "    def forward(self, inputs: Tensor) -> Tensor:\n",
        "        return inputs * inputs.sigmoid()\n",
        "\n",
        "\n",
        "class GLU(nn.Module):\n",
        "    \"\"\"\n",
        "    The gating mechanism is called Gated Linear Units (GLU), which was first introduced for natural language processing\n",
        "    in the paper “Language Modeling with Gated Convolutional Networks”\n",
        "    \"\"\"\n",
        "    def __init__(self, dim: int) -> None:\n",
        "        super(GLU, self).__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, inputs: Tensor) -> Tensor:\n",
        "        outputs, gate = inputs.chunk(2, dim=self.dim)\n",
        "        return outputs * gate.sigmoid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NVOgySDYfp-"
      },
      "source": [
        "### Convolution Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNVOlU-aNbxu"
      },
      "outputs": [],
      "source": [
        "class ConvolutionModule(nn.Module):\n",
        "\n",
        "    \"\"\"Conformer Convolution Module\n",
        "    Args:\n",
        "        dim_model: input feature dimension\n",
        "        dim_expand: output feature dimension\n",
        "        kernel_size: 1D depthwise convolution kernel size\n",
        "        Pdrop: residual dropout probability\n",
        "        stride: 1D depthwise convolution stride\n",
        "        padding: \"valid\", \"same\" or \"causal\"\n",
        "    Input: (batch size, input length, dim_model)\n",
        "    Output: (batch size, output length, dim_expand)\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim_model, dim_expand, kernel_size, Pdrop, stride, padding):\n",
        "        super(ConvolutionModule, self).__init__()\n",
        "\n",
        "        # Layers\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.LayerNorm(dim_model, eps=1e-6),\n",
        "            Transpose(1, 2),\n",
        "            Conv1d(dim_model, 2 * dim_expand, kernel_size=1),\n",
        "            GLU(dim=1),\n",
        "            Conv1d(dim_expand, dim_expand, kernel_size, stride=stride, padding=padding, groups=dim_expand),\n",
        "            nn.BatchNorm1d(dim_expand),\n",
        "            Swish(),\n",
        "            Conv1d(dim_expand, dim_expand, kernel_size=1),\n",
        "            Transpose(1, 2),\n",
        "            nn.Dropout(p=Pdrop)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xIibcFKaL26"
      },
      "source": [
        "### Attention "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vPMUk370yYD"
      },
      "outputs": [],
      "source": [
        "class RelativeSinusoidalPositionalEncoding(nn.Module):\n",
        "    \n",
        "    \"\"\"\n",
        "        Relative Sinusoidal Positional Encoding\n",
        "        Positional encoding for left context (sin) and right context (cos)\n",
        "        Total context = 2 * max_len - 1\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_len, dim_model):\n",
        "        super(RelativeSinusoidalPositionalEncoding, self).__init__()\n",
        "\n",
        "        # PE\n",
        "        pos_encoding = torch.zeros(2 * max_len - 1, dim_model)\n",
        "\n",
        "        # Positions (max_len - 1, ..., max_len - 1)\n",
        "        pos_left = torch.arange(start=max_len-1, end=0, step=-1, dtype=torch.float)\n",
        "        pos_right = torch.arange(start=0, end=-max_len, step=-1, dtype=torch.float)\n",
        "        pos = torch.cat([pos_left, pos_right], dim=0).unsqueeze(1)\n",
        "\n",
        "        # Angles\n",
        "        angles = pos / 10000**(2 * torch.arange(0, dim_model // 2, dtype=torch.float).unsqueeze(0) / dim_model)\n",
        "\n",
        "        # Rel Sinusoidal PE\n",
        "        pos_encoding[:, 0::2] = angles.sin()\n",
        "        pos_encoding[:, 1::2] = angles.cos()\n",
        "\n",
        "        pos_encoding = pos_encoding.unsqueeze(0)\n",
        "\n",
        "        self.register_buffer('pos_encoding', pos_encoding, persistent=False)\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def forward(self, batch_size=1, seq_len=None, hidden_len=0):\n",
        "\n",
        "        # (B, Th + 2*T-1, D)\n",
        "        if seq_len is not None:\n",
        "            R = self.pos_encoding[:, self.max_len - seq_len - hidden_len : self.max_len - 1  + seq_len]\n",
        "        \n",
        "        # (B, 2*Tmax-1, D)\n",
        "        else:\n",
        "            R = self.pos_encoding\n",
        "\n",
        "        return R.repeat(batch_size, 1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtlzM3rj05aL"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    \"\"\"Mutli-Head Attention Layer\n",
        "    Args:\n",
        "        dim_model: model feature dimension\n",
        "        num_heads: number of attention heads\n",
        "    References: \n",
        "        Attention Is All You Need, Vaswani et al.\n",
        "        https://arxiv.org/abs/1706.03762\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        # Attention Params\n",
        "        self.num_heads = num_heads # H\n",
        "        self.dim_model = dim_model # D\n",
        "        self.dim_head = dim_model // num_heads # d\n",
        "\n",
        "        # Linear Layers\n",
        "        self.query_layer = Linear(self.dim_model, self.dim_model)\n",
        "        self.key_layer = Linear(self.dim_model, self.dim_model)\n",
        "        self.value_layer = Linear(self.dim_model, self.dim_model)\n",
        "        self.output_layer = Linear(self.dim_model, self.dim_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "\n",
        "        \"\"\"Scaled Dot-Product Multi-Head Attention\n",
        "        Args:\n",
        "            Q: Query of shape (B, T, D)\n",
        "            K: Key of shape (B, T, D)\n",
        "            V: Value of shape (B, T, D)\n",
        "            mask: Optional position mask of shape (1 or B, 1 or H, 1 or T, 1 or T)\n",
        "        \n",
        "        Return:\n",
        "            O: Attention output of shape (B, T, D)\n",
        "            att_w: Attention weights of shape (B, H, T, T)\n",
        "        \"\"\"\n",
        "\n",
        "        # Batch size B\n",
        "        batch_size = Q.size(0)\n",
        "\n",
        "        # Linear Layers\n",
        "        Q = self.query_layer(Q)\n",
        "        K = self.key_layer(K)\n",
        "        V = self.value_layer(V)\n",
        "\n",
        "        # Reshape and Transpose (B, T, D) -> (B, H, T, d)\n",
        "        Q = Q.reshape(batch_size, -1, self.num_heads, self.dim_head).transpose(1, 2)\n",
        "        K = K.reshape(batch_size, -1, self.num_heads, self.dim_head).transpose(1, 2)\n",
        "        V = V.reshape(batch_size, -1, self.num_heads, self.dim_head).transpose(1, 2)\n",
        "\n",
        "        # Att scores (B, H, T, T)\n",
        "        att_scores = Q.matmul(K.transpose(2, 3)) / K.shape[-1]**0.5\n",
        "\n",
        "        # Apply mask\n",
        "        if mask is not None:\n",
        "            att_scores += (mask * -1e9)\n",
        "\n",
        "        # Att weights (B, H, T, T)\n",
        "        att_w = att_scores.softmax(dim=-1)\n",
        "\n",
        "        # Att output (B, H, T, d)\n",
        "        O = att_w.matmul(V)\n",
        "\n",
        "        # Transpose and Reshape (B, H, T, d) -> (B, T, D)\n",
        "        O = O.transpose(1, 2).reshape(batch_size, -1,  self.dim_model)\n",
        "\n",
        "        # Output linear layer\n",
        "        O = self.output_layer(O)\n",
        "\n",
        "        return O, att_w.detach()\n",
        "\n",
        "    def pad(self, Q, K, V, mask, chunk_size):\n",
        "\n",
        "        # Compute Overflows\n",
        "        overflow_Q = Q.size(1) % chunk_size\n",
        "        overflow_KV = K.size(1) % chunk_size\n",
        "        \n",
        "        padding_Q = chunk_size - overflow_Q if overflow_Q else 0\n",
        "        padding_KV = chunk_size - overflow_KV if overflow_KV else 0\n",
        "\n",
        "        batch_size, seq_len_KV, _ = K.size()\n",
        "\n",
        "        # Input Padding (B, T, D) -> (B, T + P, D)\n",
        "        Q = F.pad(Q, (0, 0, 0, padding_Q), value=0)\n",
        "        K = F.pad(K, (0, 0, 0, padding_KV), value=0)\n",
        "        V = F.pad(V, (0, 0, 0, padding_KV), value=0)\n",
        "\n",
        "        # Update Padding Mask\n",
        "        if mask is not None:\n",
        "\n",
        "            # (B, 1, 1, T) -> (B, 1, 1, T + P) \n",
        "            if mask.size(2) == 1:\n",
        "                mask = F.pad(mask, pad=(0, padding_KV), value=1)\n",
        "            # (B, 1, T, T) -> (B, 1, T + P, T + P)\n",
        "            else:\n",
        "                mask = F.pad(mask, pad=(0, padding_Q, 0, padding_KV), value=1)\n",
        "\n",
        "        elif padding_KV:\n",
        "\n",
        "            # None -> (B, 1, 1, T + P) \n",
        "            mask = F.pad(Q.new_zeros(batch_size, 1, 1, seq_len_KV), pad=(0, padding_KV), value=1)\n",
        "\n",
        "        return Q, K, V, mask, padding_Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAUU-B5S0rxr"
      },
      "outputs": [],
      "source": [
        "class RelPosMultiHeadSelfAttention(MultiHeadAttention):\n",
        "\n",
        "    \"\"\"Multi-Head Self-Attention Layer with Relative Sinusoidal Positional Encodings\n",
        "    Args:\n",
        "        dim_model: model feature dimension\n",
        "        num_heads: number of attention heads\n",
        "        max_pos_encoding: maximum relative distance between elements\n",
        "    References: \n",
        "        Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context, Dai et al.\n",
        "        https://arxiv.org/abs/1901.02860\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim_model, num_heads, max_pos_encoding):\n",
        "        super(RelPosMultiHeadSelfAttention, self).__init__(dim_model, num_heads)\n",
        "\n",
        "        # Position Embedding Layer\n",
        "        self.pos_layer = nn.Linear(self.dim_model, self.dim_model)\n",
        "\n",
        "        # Global content and positional bias\n",
        "        self.u = nn.Parameter(torch.Tensor(self.dim_model)) # Content bias\n",
        "        self.v = nn.Parameter(torch.Tensor(self.dim_model)) # Pos bias\n",
        "        torch.nn.init.xavier_uniform_(self.u.reshape(self.num_heads, self.dim_head)) # glorot uniform\n",
        "        torch.nn.init.xavier_uniform_(self.v.reshape(self.num_heads, self.dim_head)) # glorot uniform\n",
        "\n",
        "        # Relative Sinusoidal Positional Encodings\n",
        "        self.rel_pos_enc = RelativeSinusoidalPositionalEncoding(max_pos_encoding, self.dim_model)\n",
        "\n",
        "    def rel_to_abs(self, att_scores):\n",
        "\n",
        "        \"\"\"Relative to absolute position indexing\n",
        "        Args:\n",
        "            att_scores: absolute-by-relative indexed attention scores of shape \n",
        "            (B, H, T, Th + 2*T-1) for full context\n",
        "        Return:\n",
        "            att_scores: absolute-by-absolute indexed attention scores of shape (B, H, T, Th + T)\n",
        "        References: \n",
        "            full context:\n",
        "            Attention Augmented Convolutional Networks, Bello et al.\n",
        "            https://arxiv.org/abs/1904.09925\n",
        "        \"\"\"\n",
        "\n",
        "        # Att Scores (B, H, T, Th + 2*T-1)\n",
        "        batch_size, num_heads, seq_length1, seq_length2 = att_scores.size()\n",
        "\n",
        "        # Column Padding (B, H, T, Th + 2*T)\n",
        "        att_scores = F.pad(att_scores, pad=(0, 1), value=0)\n",
        "\n",
        "        # Flatten (B, H, TTh + 2*TT)\n",
        "        att_scores = att_scores.reshape(batch_size, num_heads, -1)\n",
        "\n",
        "        # End Padding (B, H, TTh + 2*TT + Th + T - 1)\n",
        "        att_scores = F.pad(att_scores, pad=(0, seq_length2 - seq_length1), value=0)\n",
        "\n",
        "        # Reshape (B, H, T + 1, Th + 2*T-1)\n",
        "        att_scores = att_scores.reshape(batch_size, num_heads, 1 + seq_length1, seq_length2)\n",
        "\n",
        "        # Slice (B, H, T, Th + T)\n",
        "        att_scores = att_scores[:, :, :seq_length1, seq_length1-1:]\n",
        "\n",
        "        return att_scores\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None, hidden=None):\n",
        "\n",
        "        \"\"\"Scaled Dot-Product Self-Attention with relative sinusoidal position encodings\n",
        "        Args:\n",
        "            Q: Query of shape (B, T, D)\n",
        "            K: Key of shape (B, T, D)\n",
        "            V: Value of shape (B, T, D)\n",
        "            mask: Optional position mask of shape (1 or B, 1 or H, 1 or T, 1 or T)\n",
        "            hidden: Optional Key and Value hidden states for decoding\n",
        "        \n",
        "        Return:\n",
        "            O: Attention output of shape (B, T, D)\n",
        "            att_w: Attention weights of shape (B, H, T, Th + T)\n",
        "            hidden: Key and value hidden states\n",
        "        \"\"\"\n",
        "\n",
        "        # Batch size B\n",
        "        batch_size = Q.size(0)\n",
        "\n",
        "        # Linear Layers\n",
        "        Q = self.query_layer(Q)\n",
        "        K = self.key_layer(K)\n",
        "        V = self.value_layer(V)\n",
        "\n",
        "        # Hidden State Provided\n",
        "        if hidden:\n",
        "            K = torch.cat([hidden[\"K\"], K], dim=1)\n",
        "            V = torch.cat([hidden[\"V\"], V], dim=1)\n",
        "\n",
        "        # Update Hidden State\n",
        "        hidden = {\"K\": K.detach(), \"V\": V.detach()}\n",
        "\n",
        "        # Add Bias\n",
        "        Qu = Q + self.u\n",
        "        Qv = Q + self.v\n",
        "\n",
        "        # Relative Positional Embeddings (B, Th + 2*T-1, D) / (B, Th + T, D)\n",
        "        E = self.pos_layer(self.rel_pos_enc(batch_size, Q.size(1), K.size(1) - Q.size(1)))\n",
        "\n",
        "        # Reshape and Transpose (B, T, D) -> (B, H, T, d)\n",
        "        Qu = Qu.reshape(batch_size, -1, self.num_heads, self.dim_head).transpose(1, 2)\n",
        "        Qv = Qv.reshape(batch_size, -1, self.num_heads, self.dim_head).transpose(1, 2)\n",
        "        # Reshape and Transpose (B, Th + T, D) -> (B, H, Th + T, d)\n",
        "        K = K.reshape(batch_size, -1, self.num_heads, self.dim_head).transpose(1, 2)\n",
        "        V = V.reshape(batch_size, -1, self.num_heads, self.dim_head).transpose(1, 2)\n",
        "        # Reshape and Transpose (B, Th + 2*T-1, D) -> (B, H, Th + 2*T-1, d) / (B, Th + T, D) -> (B, H, Th + T, d)\n",
        "        E = E.reshape(batch_size, -1, self.num_heads, self.dim_head).transpose(1, 2)\n",
        "\n",
        "        # att_scores (B, H, T, Th + T)\n",
        "        att_scores_K = Qu.matmul(K.transpose(2, 3))\n",
        "        att_scores_E = self.rel_to_abs(Qv.matmul(E.transpose(2, 3)))\n",
        "        att_scores = (att_scores_K + att_scores_E) / K.shape[-1]**0.5\n",
        "\n",
        "        # Apply mask\n",
        "        if mask is not None:\n",
        "            att_scores += (mask * -1e9)\n",
        "\n",
        "        # Att weights (B, H, T, Th + T)\n",
        "        att_w = att_scores.softmax(dim=-1)\n",
        "\n",
        "        # Att output (B, H, T, d)\n",
        "        O = att_w.matmul(V)\n",
        "\n",
        "        # Transpose and Reshape (B, H, T, d) -> (B, T, D)\n",
        "        O = O.transpose(1, 2).reshape(batch_size, -1,  self.dim_model)\n",
        "\n",
        "        # Output linear layer\n",
        "        O = self.output_layer(O)\n",
        "\n",
        "        return O, att_w.detach(), hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9E9oE5MALRao"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttentionModule(nn.Module):\n",
        "\n",
        "    \"\"\"Multi-Head Self-Attention Module\n",
        "    Args:\n",
        "        dim_model: model feature dimension\n",
        "        num_heads: number of attention heads\n",
        "        Pdrop: residual dropout probability\n",
        "        max_pos_encoding: maximum position\n",
        "        relative_pos_enc: whether to use relative postion embedding\n",
        "        group_size: Attention group size\n",
        "        kernel_size: Attention kernel size\n",
        "        stride: Query stride\n",
        "        linear_att: whether to use multi-head linear self-attention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, \n",
        "                 dim_model, \n",
        "                 num_heads, \n",
        "                 Pdrop,\n",
        "                 max_pos_encoding, \n",
        "                 relative_pos_enc, \n",
        "                 group_size, \n",
        "                 kernel_size, \n",
        "                 stride, \n",
        "                 linear_att):\n",
        "        super(MultiHeadSelfAttentionModule, self).__init__()\n",
        "\n",
        "        # Pre Norm\n",
        "        self.norm = nn.LayerNorm(dim_model, eps=1e-6)\n",
        "\n",
        "        # if relative_pos_enc:\n",
        "        self.mhsa = RelPosMultiHeadSelfAttention(dim_model, num_heads, max_pos_encoding)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(Pdrop)\n",
        "\n",
        "        # Module Params\n",
        "        self.rel_pos_enc = relative_pos_enc\n",
        "        self.linear_att = linear_att\n",
        "\n",
        "    def forward(self, x, mask=None, hidden=None):\n",
        "\n",
        "        # Pre Norm\n",
        "        x = self.norm(x)\n",
        "\n",
        "        # Multi-Head Self-Attention\n",
        "        x, attention, hidden = self.mhsa(x, x, x, mask, hidden)\n",
        "\n",
        "        # Dropout\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return x, attention, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMIQMmHOZkKw"
      },
      "source": [
        "## Conformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUcFtle_YaZz"
      },
      "source": [
        "#### Conformer Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pr7i12rhy6bH"
      },
      "outputs": [],
      "source": [
        "class ConformerBlock(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self, \n",
        "        dim_model, \n",
        "        dim_expand, \n",
        "        ff_ratio, \n",
        "        num_heads, \n",
        "        kernel_size, \n",
        "        att_group_size, \n",
        "        att_kernel_size,\n",
        "        linear_att,\n",
        "        Pdrop, \n",
        "        relative_pos_enc, \n",
        "        max_pos_encoding, \n",
        "        conv_stride,\n",
        "        att_stride,\n",
        "    ):\n",
        "        super(ConformerBlock, self).__init__()\n",
        "\n",
        "        # Feed Forward Module 1\n",
        "        self.feed_forward_module1 = FeedForwardModule(\n",
        "            dim_model=dim_model, \n",
        "            dim_ffn=dim_model * ff_ratio,\n",
        "            Pdrop=Pdrop, \n",
        "            act=\"swish\",\n",
        "            inner_dropout=True\n",
        "        )\n",
        "\n",
        "        # Multi-Head Self-Attention Module\n",
        "        self.multi_head_self_attention_module = MultiHeadSelfAttentionModule(\n",
        "            dim_model=dim_model, \n",
        "            num_heads=num_heads,  \n",
        "            Pdrop=Pdrop, \n",
        "            max_pos_encoding=max_pos_encoding,\n",
        "            relative_pos_enc=relative_pos_enc, \n",
        "            group_size=att_group_size,\n",
        "            kernel_size=att_kernel_size,\n",
        "            stride=att_stride,\n",
        "            linear_att=linear_att\n",
        "        )\n",
        "\n",
        "        # Convolution Module\n",
        "        self.convolution_module = ConvolutionModule(\n",
        "            dim_model=dim_model,\n",
        "            dim_expand=dim_expand,\n",
        "            kernel_size=kernel_size, \n",
        "            Pdrop=Pdrop, \n",
        "            stride=conv_stride,\n",
        "            padding=\"same\"\n",
        "        )\n",
        "\n",
        "        # Feed Forward Module 2\n",
        "        self.feed_forward_module2 = FeedForwardModule(\n",
        "            dim_model=dim_expand, \n",
        "            dim_ffn=dim_expand * ff_ratio,\n",
        "            Pdrop=Pdrop, \n",
        "            act=\"swish\",\n",
        "            inner_dropout=True\n",
        "        )\n",
        "\n",
        "        # Block Norm\n",
        "        self.norm = nn.LayerNorm(dim_expand, eps=1e-6)\n",
        "\n",
        "        # Attention Residual\n",
        "        self.att_res = nn.Sequential(\n",
        "            Transpose(1, 2),\n",
        "            nn.MaxPool1d(kernel_size=1, stride=att_stride),\n",
        "            Transpose(1, 2)\n",
        "        ) if att_stride > 1 else nn.Identity()\n",
        "\n",
        "        # Convolution Residual\n",
        "        self.conv_res = nn.Sequential(\n",
        "            Transpose(1, 2),\n",
        "            Conv1d(dim_model, dim_expand, kernel_size=1, stride=conv_stride),\n",
        "            Transpose(1, 2)\n",
        "        ) if dim_model != dim_expand else nn.Sequential(\n",
        "            Transpose(1, 2),\n",
        "            nn.MaxPool1d(kernel_size=1, stride=conv_stride),\n",
        "            Transpose(1, 2)\n",
        "        ) if conv_stride > 1 else nn.Identity()\n",
        "\n",
        "        # Bloc Stride\n",
        "        self.stride = conv_stride * att_stride\n",
        "\n",
        "    def forward(self, x, mask=None, hidden=None):\n",
        "\n",
        "        # FFN Module 1\n",
        "        x = x + 1/2 * self.feed_forward_module1(x)\n",
        "\n",
        "        # MHSA Module\n",
        "        x_att, attention, hidden = self.multi_head_self_attention_module(x, mask, hidden)\n",
        "        x = self.att_res(x) + x_att\n",
        "\n",
        "        # Conv Module\n",
        "        x = self.conv_res(x) + self.convolution_module(x)\n",
        "\n",
        "        # FFN Module 2\n",
        "        x = x + 1/2 * self.feed_forward_module2(x)\n",
        "\n",
        "        # Block Norm\n",
        "        x = self.norm(x)\n",
        "\n",
        "        return x, attention, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPB75kmXRr2M"
      },
      "source": [
        "#### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dQfVxSeRrfJ"
      },
      "outputs": [],
      "source": [
        "class AudioPreprocessing(nn.Module):\n",
        "\n",
        "    \"\"\"Audio Preprocessing\n",
        "    Computes mel-scale log filter banks spectrogram\n",
        "    Args:\n",
        "        sample_rate: Audio sample rate\n",
        "        n_fft: FFT frame size, creates n_fft // 2 + 1 frequency bins.\n",
        "        win_length_ms: FFT window length in ms, must be <= n_fft\n",
        "        hop_length_ms: length of hop between FFT windows in ms\n",
        "        n_mels: number of mel filter banks\n",
        "        normalize: whether to normalize mel spectrograms outputs\n",
        "        mean: training mean\n",
        "        std: training std\n",
        "    Shape:\n",
        "        Input: (batch_size, audio_len)\n",
        "        Output: (batch_size, n_mels, audio_len // hop_length + 1)\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, \n",
        "                 sample_rate, \n",
        "                 n_fft, \n",
        "                 win_length_ms, \n",
        "                 hop_length_ms, \n",
        "                 n_mels, \n",
        "                 normalize, \n",
        "                 mean, \n",
        "                 std):\n",
        "        super(AudioPreprocessing, self).__init__()\n",
        "        self.win_length = int(sample_rate * win_length_ms) // 1000\n",
        "        self.hop_length = int(sample_rate * hop_length_ms) // 1000\n",
        "        self.Spectrogram = torchaudio.transforms.Spectrogram(n_fft, self.win_length, self.hop_length)\n",
        "        self.MelScale = torchaudio.transforms.MelScale(n_mels, sample_rate, f_min=0, f_max=8000, n_stft=n_fft // 2 + 1)\n",
        "        self.normalize = normalize\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def forward(self, x, x_len):\n",
        "\n",
        "        # Short Time Fourier Transform (B, T) -> (B, n_fft // 2 + 1, T // hop_length + 1)\n",
        "        x = self.Spectrogram(x)\n",
        "\n",
        "        # Mel Scale (B, n_fft // 2 + 1, T // hop_length + 1) -> (B, n_mels, T // hop_length + 1)\n",
        "        x = self.MelScale(x)\n",
        "        \n",
        "        # Energy log, autocast disabled to prevent float16 overflow\n",
        "        x = (x.float() + 1e-9).log().type(x.dtype)\n",
        "\n",
        "        # Compute Sequence lengths \n",
        "        if x_len is not None:\n",
        "            x_len = torch.div(x_len, self.hop_length, rounding_mode='floor') + 1\n",
        "\n",
        "        # Normalize\n",
        "        if self.normalize:\n",
        "            x = (x - self.mean) / self.std\n",
        "\n",
        "        x = x.transpose(1,2)\n",
        "        \n",
        "        return x, x_len\n",
        "\n",
        "class SpecAugment(nn.Module):\n",
        "\n",
        "    \"\"\"Spectrogram Augmentation\n",
        "    Args:\n",
        "        spec_augment: whether to apply spec augment\n",
        "        mF: number of frequency masks\n",
        "        F: maximum frequency mask size\n",
        "        mT: number of time masks\n",
        "        pS: adaptive maximum time mask size in %\n",
        "    References:\n",
        "        SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition, Park et al.\n",
        "        https://arxiv.org/abs/1904.08779\n",
        "        SpecAugment on Large Scale Datasets, Park et al.\n",
        "        https://arxiv.org/abs/1912.05533\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, spec_augment, mF, F, mT, pS):\n",
        "        super(SpecAugment, self).__init__()\n",
        "        self.spec_augment = spec_augment\n",
        "        self.mF = mF\n",
        "        self.F = F\n",
        "        self.mT = mT\n",
        "        self.pS = pS\n",
        "\n",
        "    def forward(self, x, x_len):\n",
        "\n",
        "        # Spec Augment\n",
        "        if self.spec_augment:\n",
        "        \n",
        "            # Frequency Masking\n",
        "            for _ in range(self.mF):\n",
        "                x = torchaudio.transforms.FrequencyMasking(freq_mask_param=self.F, iid_masks=False).forward(x)\n",
        "\n",
        "            # Time Masking\n",
        "            for b in range(x.size(0)):\n",
        "                T = int(self.pS * x_len[b])\n",
        "                for _ in range(self.mT):\n",
        "                    x[b, :, :x_len[b]] = torchaudio.transforms.TimeMasking(time_mask_param=T).forward(x[b, :, :x_len[b]])\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijCuTWhrXeUD"
      },
      "source": [
        "#### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZL51wOOSXfbI"
      },
      "outputs": [],
      "source": [
        "class ConformerEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Conformer encoder first processes the input with a convolution subsampling layer and then\n",
        "    with a number of conformer blocks.\n",
        "    Args:\n",
        "        input_dim (int, optional): Dimension of input vector\n",
        "        encoder_dim (int, optional): Dimension of conformer encoder\n",
        "        num_layers (int, optional): Number of conformer blocks\n",
        "        num_attention_heads (int, optional): Number of attention heads\n",
        "        feed_forward_expansion_factor (int, optional): Expansion factor of feed forward module\n",
        "        conv_expansion_factor (int, optional): Expansion factor of conformer convolution module\n",
        "        feed_forward_dropout_p (float, optional): Probability of feed forward module dropout\n",
        "        attention_dropout_p (float, optional): Probability of attention module dropout\n",
        "        conv_dropout_p (float, optional): Probability of conformer convolution module dropout\n",
        "        conv_kernel_size (int or tuple, optional): Size of the convolving kernel\n",
        "        half_step_residual (bool): Flag indication whether to use half step residual or not\n",
        "    Inputs: inputs, input_lengths\n",
        "        - **inputs** (batch, time, dim): Tensor containing input vector\n",
        "        - **input_lengths** (batch): list of sequence input lengths\n",
        "    Returns: outputs, output_lengths\n",
        "        - **outputs** (batch, out_channels, time): Tensor produces by conformer encoder.\n",
        "        - **output_lengths** (batch): list of sequence output lengths\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            preprocessing_params,\n",
        "            input_dim: int,\n",
        "            encoder_dim: int,\n",
        "            num_layers: int = 17,\n",
        "            num_attention_heads: int = 8,\n",
        "            feed_forward_expansion_factor: int = 4,\n",
        "            conv_expansion_factor: int = 2,\n",
        "            input_dropout_p: float = 0.1,\n",
        "            feed_forward_dropout_p: float = 0.1,\n",
        "            attention_dropout_p: float = 0.1,\n",
        "            conv_dropout_p: float = 0.1,\n",
        "            conv_kernel_size: int = 31,\n",
        "            half_step_residual: bool = True,\n",
        "            decoder_dim:int = 320,\n",
        "    ):\n",
        "        super(ConformerEncoder, self).__init__()\n",
        "\n",
        "        # Audio Preprocessing\n",
        "        self.preprocessing = AudioPreprocessing(preprocessing_params[\"sample_rate\"], preprocessing_params[\"n_fft\"], preprocessing_params[\"win_length_ms\"], preprocessing_params[\"hop_length_ms\"], preprocessing_params[\"n_mels\"], preprocessing_params[\"normalize\"], preprocessing_params[\"mean\"], preprocessing_params[\"std\"])\n",
        "        \n",
        "        # Spec Augment\n",
        "        self.augment = SpecAugment(preprocessing_params[\"spec_augment\"], preprocessing_params[\"mF\"], preprocessing_params[\"F\"], preprocessing_params[\"mT\"], preprocessing_params[\"pS\"])\n",
        "\n",
        "        self.conv_subsample = Conv2dSubampling(in_channels=1, \n",
        "                                               out_channels=encoder_dim,\n",
        "                                               num_conv_layers=2)\n",
        "        \n",
        "        self.input_projection = nn.Sequential(\n",
        "            Linear(encoder_dim * (preprocessing_params[\"n_mels\"] // 2**2), encoder_dim),\n",
        "            nn.Dropout(p=input_dropout_p),\n",
        "        )\n",
        "\n",
        "        self.layers = nn.ModuleList([ConformerBlock(\n",
        "            dim_model=encoder_dim,\n",
        "            dim_expand=encoder_dim,\n",
        "            ff_ratio=feed_forward_expansion_factor,\n",
        "            num_heads=num_attention_heads, \n",
        "            kernel_size=conv_kernel_size, \n",
        "            att_group_size=1,\n",
        "            att_kernel_size=None,\n",
        "            linear_att=False,\n",
        "            Pdrop=0.1, \n",
        "            relative_pos_enc=True, \n",
        "            max_pos_encoding=10000,\n",
        "            conv_stride=1,\n",
        "            att_stride=1,\n",
        "        ) for block_id in range(num_layers)])\n",
        "\n",
        "        # self.linear = nn.Linear(encoder_dim, decoder_dim)\n",
        "\n",
        "    def count_parameters(self) -> int:\n",
        "        \"\"\" Count parameters of encoder \"\"\"\n",
        "        return sum([p.numel for p in self.parameters()])\n",
        "\n",
        "    def update_dropout(self, dropout_p: float) -> None:\n",
        "        \"\"\" Update dropout probability of encoder \"\"\"\n",
        "        for name, child in self.named_children():\n",
        "            if isinstance(child, nn.Dropout):\n",
        "                child.p = dropout_p\n",
        "\n",
        "    def forward(self, inputs: Tensor, input_lengths: Tensor, training=False) -> Tuple[Tensor, Tensor]:\n",
        "        \"\"\"\n",
        "        Forward propagate a `inputs` for  encoder training.\n",
        "        Args:\n",
        "            inputs (torch.FloatTensor): A input sequence passed to encoder. Typically for inputs this will be a padded\n",
        "                `FloatTensor` of size ``(batch, seq_length, dimension)``.\n",
        "            input_lengths (torch.LongTensor): The length of input tensor. ``(batch)``\n",
        "        Returns:\n",
        "            (Tensor, Tensor)\n",
        "            * outputs (torch.FloatTensor): A output sequence of encoder. `FloatTensor` of size\n",
        "                ``(batch, seq_length, dimension)``\n",
        "            * output_lengths (torch.LongTensor): The length of output tensor. ``(batch)``\n",
        "        \"\"\"\n",
        "\n",
        "        # Audio Preprocessing\n",
        "        inputs, input_lengths = self.preprocessing(inputs, input_lengths)\n",
        "\n",
        "        # Spec Augment\n",
        "        if training:\n",
        "            inputs = self.augment(inputs, input_lengths)\n",
        "        outputs, output_lengths = self.conv_subsample(inputs, input_lengths)\n",
        "        outputs = self.input_projection(outputs)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            outputs, _, _ = layer(outputs)\n",
        "        \n",
        "        # outputs = self.linear(outputs)\n",
        "        return outputs, output_lengths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8x59ffNxVyC9"
      },
      "source": [
        "#### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxa8NeCMVsdk"
      },
      "outputs": [],
      "source": [
        "class DecoderRNNT(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder of RNN-Transducer\n",
        "    Args:\n",
        "        num_classes (int): number of classification\n",
        "        hidden_state_dim (int, optional): hidden state dimension of decoder (default: 512)\n",
        "        output_dim (int, optional): output dimension of encoder and decoder (default: 512)\n",
        "        num_layers (int, optional): number of decoder layers (default: 1)\n",
        "        rnn_type (str, optional): type of rnn cell (default: lstm)\n",
        "        sos_id (int, optional): start of sentence identification\n",
        "        eos_id (int, optional): end of sentence identification\n",
        "        dropout_p (float, optional): dropout probability of decoder\n",
        "    Inputs: inputs, input_lengths\n",
        "        inputs (torch.LongTensor): A target sequence passed to decoder. `IntTensor` of size ``(batch, seq_length)``\n",
        "        input_lengths (torch.LongTensor): The length of input tensor. ``(batch)``\n",
        "        hidden_states (torch.FloatTensor): A previous hidden state of decoder. `FloatTensor` of size\n",
        "            ``(batch, seq_length, dimension)``\n",
        "    Returns:\n",
        "        (Tensor, Tensor):\n",
        "        * decoder_outputs (torch.FloatTensor): A output sequence of decoder. `FloatTensor` of size\n",
        "            ``(batch, seq_length, dimension)``\n",
        "        * hidden_states (torch.FloatTensor): A hidden state of decoder. `FloatTensor` of size\n",
        "            ``(batch, seq_length, dimension)``\n",
        "    \"\"\"\n",
        "    supported_rnns = {\n",
        "        'lstm': nn.LSTM,\n",
        "        'gru': nn.GRU,\n",
        "        'rnn': nn.RNN,\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            num_classes: int,\n",
        "            hidden_state_dim: int,\n",
        "            output_dim: int,\n",
        "            num_layers: int,\n",
        "            rnn_type: str = 'lstm',\n",
        "            sos_id: int = 1,\n",
        "            eos_id: int = 2,\n",
        "            dropout_p: float = 0.2,\n",
        "    ):\n",
        "        super(DecoderRNNT, self).__init__()\n",
        "        # self.hidden_state_dim = hidden_state_dim\n",
        "        # self.sos_id = sos_id\n",
        "        # self.eos_id = eos_id\n",
        "        # self.embedding = nn.Embedding(num_classes, hidden_state_dim, padding_idx=0)\n",
        "        # rnn_cell = self.supported_rnns[rnn_type.lower()]\n",
        "        # self.rnn = rnn_cell(\n",
        "        #     input_size=hidden_state_dim,\n",
        "        #     hidden_size=hidden_state_dim,\n",
        "        #     num_layers=num_layers,\n",
        "        #     bias=True,\n",
        "        #     batch_first=True,\n",
        "        #     dropout=dropout_p,\n",
        "        #     bidirectional=False,\n",
        "        # )\n",
        "        # self.out_proj = Linear(hidden_state_dim, output_dim)\n",
        "\n",
        "        #self.embedding = nn.Embedding(num_classes, hidden_state_dim, padding_idx=0)\n",
        "        self.embedding = nn.Embedding(num_classes, hidden_state_dim, padding_idx=0)\n",
        "\n",
        "        rnn_cell = self.supported_rnns[rnn_type.lower()]\n",
        "        self.rnn = rnn_cell(\n",
        "            input_size=hidden_state_dim,\n",
        "            hidden_size=hidden_state_dim,\n",
        "            num_layers=num_layers,\n",
        "            #bias=True,\n",
        "            batch_first=True,\n",
        "            #dropout=dropout_p,\n",
        "            bidirectional=False,\n",
        "        )\n",
        "        self.out_proj = Linear(hidden_state_dim, output_dim)\n",
        "\n",
        "    def count_parameters(self) -> int:\n",
        "        \"\"\" Count parameters of encoder \"\"\"\n",
        "        return sum([p.numel for p in self.parameters()])\n",
        "\n",
        "    def update_dropout(self, dropout_p: float) -> None:\n",
        "        \"\"\" Update dropout probability of encoder \"\"\"\n",
        "        for name, child in self.named_children():\n",
        "            if isinstance(child, nn.Dropout):\n",
        "                child.p = dropout_p\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            inputs: Tensor,\n",
        "            input_lengths: Tensor = None,\n",
        "            hidden_states: Tensor = None,\n",
        "    ) -> Tuple[Tensor, Tensor]:\n",
        "        \"\"\"\n",
        "        Forward propage a `inputs` (targets) for training.\n",
        "        Args:\n",
        "            inputs (torch.LongTensor): A target sequence passed to decoder. `IntTensor` of size ``(batch, seq_length)``\n",
        "            input_lengths (torch.LongTensor): The length of input tensor. ``(batch)``\n",
        "            hidden_states (torch.FloatTensor): A previous hidden state of decoder. `FloatTensor` of size\n",
        "                ``(batch, seq_length, dimension)``\n",
        "        Returns:\n",
        "            (Tensor, Tensor):\n",
        "            * decoder_outputs (torch.FloatTensor): A output sequence of decoder. `FloatTensor` of size\n",
        "                ``(batch, seq_length, dimension)``\n",
        "            * hidden_states (torch.FloatTensor): A hidden state of decoder. `FloatTensor` of size\n",
        "                ``(batch, seq_length, dimension)``\n",
        "        \"\"\"\n",
        "\n",
        "        # Sequence Embedding (B, U + 1) -> (B, U + 1, D)\n",
        "        embedded = self.embedding(inputs)\n",
        "\n",
        "        if input_lengths is not None:\n",
        "            embedded = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "            outputs, hidden_states = self.rnn(embedded, hidden_states)\n",
        "            outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
        "            outputs = self.out_proj(outputs)\n",
        "\n",
        "        else:\n",
        "            outputs, hidden_states = self.rnn(embedded, hidden_states)\n",
        "            outputs = self.out_proj(outputs)\n",
        "\n",
        "        return outputs, hidden_states"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCBPWoujeucW"
      },
      "source": [
        "#### Conformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBmuUZP4OoPg"
      },
      "outputs": [],
      "source": [
        "class Conformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Conformer: Convolution-augmented Transformer for Speech Recognition\n",
        "    The paper used a one-lstm Transducer decoder, currently still only implemented\n",
        "    the conformer encoder shown in the paper.\n",
        "    Args:\n",
        "        hparams: HParams\n",
        "    Inputs: inputs\n",
        "        - **inputs** (batch, time, dim): Tensor containing input vector\n",
        "        - **input_lengths** (batch): list of sequence input lengths\n",
        "    Returns: outputs, output_lengths\n",
        "        - **outputs** (batch, out_channels, time): Tensor produces by conformer.\n",
        "        - **output_lengths** (batch): list of sequence output lengths\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            hparams\n",
        "    ) -> None:\n",
        "        super(Conformer, self).__init__()\n",
        "        self.encoder = ConformerEncoder(\n",
        "            preprocessing_params=hparams.preprocessing_params,\n",
        "            input_dim=hparams.input_dim,\n",
        "            encoder_dim=hparams.encoder_dim,\n",
        "            num_layers=hparams.num_encoder_layers,\n",
        "            num_attention_heads=hparams.num_attention_heads,\n",
        "            feed_forward_expansion_factor=hparams.feed_forward_expansion_factor,\n",
        "            conv_expansion_factor=hparams.conv_expansion_factor,\n",
        "            input_dropout_p=hparams.input_dropout_p,\n",
        "            feed_forward_dropout_p=hparams.feed_forward_dropout_p,\n",
        "            attention_dropout_p=hparams.attention_dropout_p,\n",
        "            conv_dropout_p=hparams.conv_dropout_p,\n",
        "            conv_kernel_size=hparams.conv_kernel_size,\n",
        "            half_step_residual=hparams.half_step_residual,\n",
        "        )\n",
        "        self.decoder = DecoderRNNT(\n",
        "            num_classes=hparams.num_classes,\n",
        "            hidden_state_dim=hparams.decoder_dim,\n",
        "            output_dim=hparams.encoder_dim,\n",
        "            # output_dim=hparams.decoder_dim,\n",
        "            num_layers=hparams.num_decoder_layers,\n",
        "            rnn_type=hparams.decoder_rnn_type,\n",
        "            dropout_p=hparams.decoder_dropout_p,\n",
        "        )\n",
        "\n",
        "        # se concat hparams.encoder_dim * 2, se sum hparams.encoder_dim\n",
        "        self.act_fn = nn.Tanh()\n",
        "        # self.fc = Linear(hparams.encoder_dim * 2, hparams.num_classes, bias=False)\n",
        "        # self.fc = Linear(hparams.decoder_dim, hparams.num_classes, bias=False)\n",
        "        self.fc = Linear(hparams.encoder_dim, hparams.num_classes, bias=False)\n",
        "\n",
        "    def set_encoder(self, encoder):\n",
        "        \"\"\" Setter for encoder \"\"\"\n",
        "        self.encoder = encoder\n",
        "\n",
        "    def set_decoder(self, decoder):\n",
        "        \"\"\" Setter for decoder \"\"\"\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def count_parameters(self) -> int:\n",
        "        \"\"\" Count parameters of encoder \"\"\"\n",
        "        num_encoder_parameters = self.encoder.count_parameters()\n",
        "        num_decoder_parameters = self.decoder.count_parameters()\n",
        "        return num_encoder_parameters + num_decoder_parameters\n",
        "\n",
        "    def update_dropout(self, dropout_p) -> None:\n",
        "        \"\"\" Update dropout probability of model \"\"\"\n",
        "        self.encoder.update_dropout(dropout_p)\n",
        "        self.decoder.update_dropout(dropout_p)\n",
        "\n",
        "    def joint(self, encoder_outputs: Tensor, decoder_outputs: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Joint `encoder_outputs` and `decoder_outputs`.\n",
        "        Args:\n",
        "            encoder_outputs (torch.FloatTensor): A output sequence of encoder. `FloatTensor` of size\n",
        "                ``(batch, seq_length, dimension)``\n",
        "            decoder_outputs (torch.FloatTensor): A output sequence of decoder. `FloatTensor` of size\n",
        "                ``(batch, seq_length, dimension)``\n",
        "        Returns:\n",
        "            * outputs (torch.FloatTensor): outputs of joint `encoder_outputs` and `decoder_outputs`..\n",
        "        \"\"\"\n",
        "        if encoder_outputs.dim() == 3 and decoder_outputs.dim() == 3:\n",
        "            input_length = encoder_outputs.size(1)\n",
        "            target_length = decoder_outputs.size(1)\n",
        "\n",
        "            encoder_outputs = encoder_outputs.unsqueeze(2)\n",
        "            decoder_outputs = decoder_outputs.unsqueeze(1) \n",
        "\n",
        "            encoder_outputs = encoder_outputs.repeat([1, 1, target_length, 1])\n",
        "            decoder_outputs = decoder_outputs.repeat([1, input_length, 1, 1])\n",
        "\n",
        "        # Loro fanno sum e poi usano tanh activation function  \n",
        "        # outputs = torch.cat((encoder_outputs, decoder_outputs), dim=-1)\n",
        "        outputs = encoder_outputs + decoder_outputs\n",
        "        outputs = self.act_fn(outputs)\n",
        "        outputs = self.fc(outputs)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            inputs: Tensor,\n",
        "            input_lengths: Tensor,\n",
        "            targets: Tensor,\n",
        "            target_lengths: Tensor,\n",
        "            training = False\n",
        "    ) -> Tensor:\n",
        "        \"\"\"\n",
        "        Forward propagate a `inputs` and `targets` pair for training.\n",
        "        Args:\n",
        "            inputs (torch.FloatTensor): A input sequence passed to encoder. Typically for inputs this will be a padded\n",
        "                `FloatTensor` of size ``(batch, seq_length, dimension)``.\n",
        "            input_lengths (torch.LongTensor): The length of input tensor. ``(batch)``\n",
        "            targets (torch.LongTensr): A target sequence passed to decoder. `IntTensor` of size ``(batch, seq_length)``\n",
        "            target_lengths (torch.LongTensor): The length of target tensor. ``(batch)``\n",
        "        Returns:\n",
        "            * predictions (torch.FloatTensor): Result of model predictions.\n",
        "        \"\"\"\n",
        "        encoder_outputs, encoder_output_len = self.encoder(inputs, input_lengths, training=training)\n",
        "        \n",
        "        targets = torch.nn.functional.pad(targets, pad=(1, 0, 0, 0), value=0)\n",
        "        target_lengths = target_lengths + 1\n",
        "        \n",
        "        decoder_outputs, _ = self.decoder(targets, target_lengths)\n",
        "        \n",
        "        outputs = self.joint(encoder_outputs, decoder_outputs)\n",
        "        # linear for encoder and decoder ALREADY MADE IN ENC AND DEC\n",
        "\n",
        "        return outputs, encoder_output_len\n",
        "\n",
        "    # @torch.no_grad()\n",
        "    # def decode(self, encoder_output: Tensor, max_length: int) -> Tensor:\n",
        "    #     \"\"\"\n",
        "    #     Decode `encoder_outputs`.\n",
        "    #     Args:\n",
        "    #         encoder_output (torch.FloatTensor): A output sequence of encoder. `FloatTensor` of size\n",
        "    #             ``(seq_length, dimension)``\n",
        "    #         max_length (int): max decoding time step\n",
        "    #     Returns:\n",
        "    #         * predicted_log_probs (torch.FloatTensor): Log probability of model predictions.\n",
        "    #     \"\"\"\n",
        "    #     pred_tokens, hidden_state = list(), None\n",
        "    #     decoder_input = encoder_output.new_tensor([[self.decoder.sos_id]], dtype=torch.long)\n",
        "\n",
        "    #     for t in range(max_length):\n",
        "    #         decoder_output, hidden_state = self.decoder(decoder_input, hidden_states=hidden_state)\n",
        "    #         step_output = self.joint(encoder_output[t].view(-1), decoder_output.view(-1))\n",
        "    #         step_output = step_output.softmax(dim=0)\n",
        "    #         pred_token = step_output.argmax(dim=0)\n",
        "    #         pred_token = int(pred_token.item())\n",
        "    #         pred_tokens.append(pred_token)\n",
        "    #         decoder_input = step_output.new_tensor([[pred_token]], dtype=torch.long)\n",
        "\n",
        "    #     return torch.LongTensor(pred_tokens)\n",
        "\n",
        "    # @torch.no_grad()\n",
        "    # def recognize(self, inputs: Tensor, input_lengths: Tensor):\n",
        "    #     \"\"\"\n",
        "    #     Recognize input speech. This method consists of the forward of the encoder and the decode() of the decoder.\n",
        "    #     Args:\n",
        "    #         inputs (torch.FloatTensor): A input sequence passed to encoder. Typically for inputs this will be a padded\n",
        "    #             `FloatTensor` of size ``(batch, seq_length, dimension)``.\n",
        "    #         input_lengths (torch.LongTensor): The length of input tensor. ``(batch)``\n",
        "    #     Returns:\n",
        "    #         * predictions (torch.FloatTensor): Result of model predictions.\n",
        "    #     \"\"\"\n",
        "    #     outputs = list()\n",
        "\n",
        "    #     encoder_outputs, output_lengths = self.encoder(inputs, input_lengths)\n",
        "    #     max_length = encoder_outputs.size(1)\n",
        "\n",
        "    #     for encoder_output in encoder_outputs:\n",
        "    #         decoded_seq = self.decode(encoder_output, max_length)\n",
        "    #         outputs.append(decoded_seq)\n",
        "\n",
        "    #     outputs = torch.stack(outputs, dim=1).transpose(0, 1)\n",
        "\n",
        "    #     return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XS4MLT40OgN_"
      },
      "source": [
        "# Hyperparameters and Model initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PklAJBzGOl6W"
      },
      "outputs": [],
      "source": [
        "class HParams():\n",
        "    \"\"\"       \n",
        "    HParams: Hyperparameters needed to initialize the Conformer model     \n",
        "    Args:\n",
        "        num_classes (int): Number of classification classes\n",
        "        input_dim (int, optional): Dimension of input vector\n",
        "        encoder_dim (int, optional): Dimension of conformer encoder\n",
        "        decoder_dim (int, optional): Dimension of conformer decoder\n",
        "        num_encoder_layers (int, optional): Number of conformer blocks\n",
        "        num_decoder_layers (int, optional): Number of decoder layers\n",
        "        decoder_rnn_type (str, optional): type of RNN cell\n",
        "        num_attention_heads (int, optional): Number of attention heads\n",
        "        feed_forward_expansion_factor (int, optional): Expansion factor of feed forward module\n",
        "        conv_expansion_factor (int, optional): Expansion factor of conformer convolution module\n",
        "        feed_forward_dropout_p (float, optional): Probability of feed forward module dropout\n",
        "        attention_dropout_p (float, optional): Probability of attention module dropout\n",
        "        conv_dropout_p (float, optional): Probability of conformer convolution module dropout\n",
        "        decoder_dropout_p (float, optional): Probability of conformer decoder dropout\n",
        "        conv_kernel_size (int or tuple, optional): Size of the convolving kernel\n",
        "        half_step_residual (bool): Flag indication whether to use half step residual or not\n",
        "    \"\"\"\n",
        "    \n",
        "    num_classes: int = 256\n",
        "    input_dim: int = 80\n",
        "    encoder_dim = 144\n",
        "    decoder_dim = 320\n",
        "    num_encoder_layers = 16\n",
        "    num_decoder_layers = 1\n",
        "    num_attention_heads = 4\n",
        "    feed_forward_expansion_factor: int = 4\n",
        "    conv_expansion_factor: int = 2\n",
        "    input_dropout_p: float = 0.1\n",
        "    feed_forward_dropout_p: float = 0.1\n",
        "    attention_dropout_p: float = 0.1\n",
        "    conv_dropout_p: float = 0.1\n",
        "    decoder_dropout_p: float = 0.1\n",
        "    conv_kernel_size: int = 31\n",
        "    half_step_residual: bool = True\n",
        "    decoder_rnn_type: str = \"lstm\"\n",
        "\n",
        "    preprocessing_params = {\n",
        "        \"sample_rate\": 16000,\n",
        "        \"win_length_ms\": 25,\n",
        "        \"hop_length_ms\": 10,\n",
        "        \"n_fft\": 512,\n",
        "        \"n_mels\": 80,\n",
        "        \"normalize\": False,\n",
        "        \"mean\": -5.6501,\n",
        "        \"std\": 4.2280,\n",
        "\n",
        "        \"spec_augment\": True,\n",
        "        \"mF\": 2,\n",
        "        \"F\": 27,\n",
        "        \"mT\": 5,\n",
        "        \"pS\": 0.05\n",
        "    }\n",
        "\n",
        "    training_params = {\n",
        "        \"epochs\": 20,\n",
        "        # \"batch_size\": 16,\n",
        "        \"accumulated_steps\": 4,\n",
        "\n",
        "        \"beta1\": 0.9,\n",
        "        \"beta2\": 0.98,\n",
        "        \"eps\": 1e-9,\n",
        "        \"weight_decay\": 1e-6,\n",
        "        \"lr\": 0.05/math.sqrt(encoder_dim),\n",
        "\n",
        "        \"schedule_dim\": 144,\n",
        "        \"warmup_steps\": 10000,\n",
        "        \"K\": 2\n",
        "    }\n",
        "\n",
        "params = HParams()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWa1W2SDao4G"
      },
      "outputs": [],
      "source": [
        "model = Conformer(params).to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Sbxcst4vFLS",
        "outputId": "288d8e41-bce9-402d-edf8-0e5f8724034d"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(PATH + \"Conformer.pth\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8nfAkBWOpGz"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ME8miSqVZl93"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(params=model.parameters(), \n",
        "                             lr=params.training_params[\"lr\"], \n",
        "                             betas=(params.training_params[\"beta1\"], params.training_params[\"beta2\"]), \n",
        "                             eps=params.training_params[\"eps\"], \n",
        "                             weight_decay=params.training_params[\"weight_decay\"])\n",
        "\n",
        "scheduler = transformers.get_linear_schedule_with_warmup(optimizer=optimizer, \n",
        "                                                         num_warmup_steps=params.training_params[\"warmup_steps\"],\n",
        "                                                         num_training_steps=500, \n",
        "                                                         last_epoch = -1) \n",
        "\n",
        "#optimizer.load_state_dict(torch.load(PATH + \"Contextnet_optimizer.pth\"))\n",
        "#scheduler.load_state_dict(torch.load(PATH + \"Contextnet_scheduler.pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaNHjyVLPKfr"
      },
      "outputs": [],
      "source": [
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "path = PATH + \"Conformer\"\n",
        "\n",
        "trainer = Trainer(model, optimizer, scheduler, scaler, path)\n",
        "\n",
        "#trainer.train(train_dataloader, dev_dataloader, epochs=15)\n",
        "trainer.evaluate(dev_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDiuOVcpeNzt"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnUgfLBEeNEs"
      },
      "outputs": [],
      "source": [
        "def test(test_dataset, path):\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    speech_true = []\n",
        "    speech_pred = []\n",
        "    total_wer = 0.0\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with open(path, \"w\") as f:\n",
        "      writer = csv.writer(f)\n",
        "      # Evaluation Loop\n",
        "      for step, batch in enumerate(tqdm(test_dataset)):\n",
        "\n",
        "          inputs, targets, input_len, target_len = batch\n",
        "\n",
        "          # Sequence Prediction\n",
        "          with torch.no_grad():\n",
        "\n",
        "              outputs_pred = greedy_search_decoding(inputs, input_len)\n",
        "\n",
        "          # Sequence Truth\n",
        "          outputs_true = tokenizer.decode(targets.tolist())\n",
        "\n",
        "          # Compute Batch wer and Update total wer\n",
        "          batch_wer = jiwer.wer(outputs_true, outputs_pred, standardize=True)\n",
        "          total_wer += batch_wer\n",
        "\n",
        "          # Update String lists\n",
        "          speech_true += outputs_true\n",
        "          speech_pred += outputs_pred\n",
        "\n",
        "          # Prediction Verbose\n",
        "          print(\"Groundtruths :\\n\", outputs_true)\n",
        "          print(\"Predictions :\\n\", outputs_pred)\n",
        "\n",
        "          # Eval Loss\n",
        "          with torch.no_grad():\n",
        "              pred, pred_len = model.forward(inputs, input_len, targets, target_len)\n",
        "              batch_loss = warp_rnnt.rnnt_loss(\n",
        "                                  log_probs=torch.nn.functional.log_softmax(pred, dim=-1),\n",
        "                                  labels=targets.int(),\n",
        "                                  frames_lengths=pred_len.int(),\n",
        "                                  labels_lengths=target_len.int(),\n",
        "                                  average_frames=False,\n",
        "                                  reduction='mean',\n",
        "                                  blank=0,\n",
        "                                  gather=True)\n",
        "              total_loss += batch_loss\n",
        "\n",
        "          # Step print\n",
        "          print(\"\\nmean batch wer {:.2f}% - batch wer: {:.2f}% - mean loss {:.4f} - batch loss: {:.4f}\".format(100 * total_wer / (step + 1), 100 * batch_wer, total_loss / (step + 1), batch_loss))\n",
        "\n",
        "          writer.writerow([outputs_true,outputs_pred,100 * batch_wer,batch_loss])\n",
        "\n",
        "    # Compute wer\n",
        "    if total_wer / test_dataset.__len__() > 1:\n",
        "        wer = 1\n",
        "    else:\n",
        "        wer = jiwer.wer(speech_true, speech_pred, standardize=True)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = total_loss / test_dataset.__len__()\n",
        "\n",
        "    return wer, speech_true, speech_pred, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbP2KE-IoDLn"
      },
      "outputs": [],
      "source": [
        "test_clean_dataset = LibriSpeechDataset(\"test-clean\", tokenizer)\n",
        "test_other_dataset = LibriSpeechDataset(\"test-other\", tokenizer)\n",
        "\n",
        "test_clean_dataloader = torch.utils.data.DataLoader(test_clean_dataset,\n",
        "                                                    batch_size=4,\n",
        "                                                    shuffle=False,\n",
        "                                                    collate_fn=collate_fn,\n",
        "                                                    drop_last=True)\n",
        "\n",
        "test_other_dataloader = torch.utils.data.DataLoader(test_other_dataset,\n",
        "                                                    batch_size=4,\n",
        "                                                    shuffle=False,\n",
        "                                                    collate_fn=collate_fn,\n",
        "                                                    drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p35Yjs1b96UD"
      },
      "outputs": [],
      "source": [
        "path_test_clean = PATH + \"Conformer_test_clean.csv\"\n",
        "wer_clean, _, _, loss_clean = test(test_clean_dataloader, path_test_clean)\n",
        "print(wer_clean, loss_clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38ndYzc-wODe"
      },
      "outputs": [],
      "source": [
        "path_test_other = PATH + \"Conformer_test_other.csv\"\n",
        "wer_clean, _, _, loss_clean = test(test_other_dataloader, path_test_other)\n",
        "print(wer_clean,loss_clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7Ej5NfC2LWM"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "with open(path+\".tsv\", \"r\") as f:\n",
        "    reader = csv.reader(f, delimiter=\"\\t\")\n",
        "\n",
        "    p = []\n",
        "    for i, row in enumerate(reader):\n",
        "        if i < 4:\n",
        "            continue\n",
        "        p.append(float(row[1]))\n",
        "\n",
        "    print(p)\n",
        "    plt.plot(p)\n",
        "\n",
        "path2 = PATH + \"Contextnet\"\n",
        "\n",
        "with open(path2+\".tsv\", \"r\") as f:\n",
        "    reader = csv.reader(f, delimiter=\"\\t\")\n",
        "\n",
        "    p = []\n",
        "    for row in reader:\n",
        "        p.append(float(row[1]))\n",
        "\n",
        "    print(p)\n",
        "    plt.plot(p)\n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"WER\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ciykJgaBWIIB",
        "YCCsv_kwfKHw",
        "q8TFEHrKZ4aA",
        "4knMA-P4ZBsj",
        "_NVOgySDYfp-",
        "1xIibcFKaL26",
        "bMIQMmHOZkKw",
        "OUcFtle_YaZz",
        "SPB75kmXRr2M",
        "ijCuTWhrXeUD",
        "8x59ffNxVyC9",
        "GCBPWoujeucW"
      ],
      "name": "MIO CONFORMER SpeechRecognition_prova.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
