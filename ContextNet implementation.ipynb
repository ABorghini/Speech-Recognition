{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-y50ZDUAXw3K"
      },
      "source": [
        "# ContextNet implementation\n",
        "Code inspired by: https://github.com/upskyy/ContextNet\n",
        "\n",
        "Author: Borghini Alessia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4LL4ypOXrr0"
      },
      "source": [
        "Check the notebook GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1BMxlJU6zNP",
        "outputId": "68de8947-865b-4c42-fa00-1fa64af20c34"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBajfdoFkB7p",
        "outputId": "8a338d34-132e-4401-af37-8d25893ecdc8"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive \n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9M0AHU1kJ04"
      },
      "source": [
        "# Import prerequirements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOgHoje1Xl1i"
      },
      "source": [
        "Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOOv3u_RSghU",
        "outputId": "48e7c23c-bc20-4720-afee-82c7fce39e97"
      },
      "outputs": [],
      "source": [
        "!pip install -r drive/My\\ Drive/Speech_Recognition/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvqbrJ53XfmR"
      },
      "source": [
        "Import libraries and set seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAfXnCSvjyto",
        "outputId": "9ed78fe8-798b-49e3-83e5-cfdccc584cce"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch \n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader, Sampler\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchaudio\n",
        "\n",
        "import numpy as np\n",
        "import librosa\n",
        "from tqdm import tqdm\n",
        "from typing import Tuple, Optional\n",
        "import math\n",
        "\n",
        "import warp_rnnt\n",
        "\n",
        "import jiwer\n",
        "import sentencepiece\n",
        "\n",
        "import transformers\n",
        "\n",
        "import os\n",
        "import csv\n",
        "\n",
        "PATH = \"drive/MyDrive/Speech_Recognition/\"\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsriWRlC0tZ1"
      },
      "source": [
        "# Librispeech Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WXnIYd_qKOf"
      },
      "outputs": [],
      "source": [
        "os.mkdir(\"datasets\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O17_StTrXb6d"
      },
      "source": [
        "Downloading LibriSpeech dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPD-_m5pX2gO",
        "outputId": "90c6c6dc-108d-4524-eda2-9a0f99c562ce"
      },
      "outputs": [],
      "source": [
        "# Download LibriSPeech train-clean-100 subset\n",
        "# !cd datasets && wget https://www.openslr.org/resources/12/train-clean-100.tar.gz && tar xzf train-clean-100.tar.gz\n",
        "\n",
        "# Download LibriSPeech train-clean-360 subset\n",
        "# !cd datasets && wget https://www.openslr.org/resources/12/train-clean-360.tar.gz && tar xzf train-clean-360.tar.gz\n",
        "\n",
        "# Download LibriSPeech dev-clean subset\n",
        "!cd datasets && wget https://www.openslr.org/resources/12/dev-clean.tar.gz && tar xzf dev-clean.tar.gz\n",
        "\n",
        "# Download LibriSPeech dev-other subset\n",
        "# !cd datasets && wget https://www.openslr.org/resources/12/dev-other.tar.gz && tar xzf dev-other.tar.gz\n",
        "\n",
        "# Download LibriSPeech test-clean subset\n",
        "# !cd datasets && wget https://www.openslr.org/resources/12/test-clean.tar.gz && tar xzf test-clean.tar.gz\n",
        "\n",
        "# Download LibriSPeech test-other subset\n",
        "# !cd datasets && wget https://www.openslr.org/resources/12/test-other.tar.gz && tar xzf test-other.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCykbYKpXRf7"
      },
      "source": [
        "LibriSpeech dataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQujR2RTmz1L"
      },
      "outputs": [],
      "source": [
        "import glob \n",
        "\n",
        "class LibriSpeechDataset():\n",
        "    def __init__(self, split, tokenizer):\n",
        "\n",
        "        print(\"Creating dataset..\")\n",
        "        self.data_names = glob.glob(\"datasets/LibriSpeech/\" + split + \"*/*/*/*.flac\")\n",
        "        self.vocab_type = \"bpe\"\n",
        "        self.vocab_size = 1000\n",
        "\n",
        "        # if split == \"train-clean-100\":\n",
        "        label_paths = []\n",
        "        sentences = []\n",
        "\n",
        "        for file_path in glob.glob(\"datasets/LibriSpeech/\" + split + \"/*/*/*.txt\"):\n",
        "            for line in open(file_path, \"r\").readlines():\n",
        "                label_paths.append(file_path.replace(file_path.split(\"/\")[-1], \"\") + line.split()[0] + \".\" + self.vocab_type + \"_\" + str(self.vocab_size))\n",
        "                sentences.append(line[len(line.split()[0]) + 1:-1].lower())\n",
        "\n",
        "        for (sentence, label_path) in tqdm(zip(sentences, label_paths)):\n",
        "            # Tokenize and Save label\n",
        "            label = torch.LongTensor(tokenizer.encode(sentence))\n",
        "            torch.save(label, label_path)\n",
        "\n",
        "            # Save Audio length\n",
        "            audio_length = torchaudio.load(label_path.split(\".\")[0] + \".flac\")[0].size(1)\n",
        "            torch.save(audio_length, label_path.split(\".\")[0] + \".flac_len\")\n",
        "\n",
        "            # Save Label length\n",
        "            label_length = label.size(0)\n",
        "            torch.save(label_length, label_path + \"_len\")\n",
        "                \n",
        "        print(\"Done.\")\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return [torchaudio.load(self.data_names[i])[0], \n",
        "                torch.load(self.data_names[i].split(\".flac\")[0] + \".\" + self.vocab_type + \"_\" + str(self.vocab_size)),\n",
        "                torch.load(self.data_names[i] + \"_len\"),\n",
        "                torch.load(self.data_names[i].split(\".flac\")[0] + \".\" + self.vocab_type + \"_\" + str(self.vocab_size) + \"_len\")]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlMnTkTpXNOq"
      },
      "source": [
        "Training the tokenizer (uncomment to create a new tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTuHcp1SztS9"
      },
      "outputs": [],
      "source": [
        "# corpus_path = \"datasets/LibriSpeech/train-clean-100_corpus.txt\"\n",
        "\n",
        "# # Create Corpus File\n",
        "# if not os.path.isfile(corpus_path):\n",
        "#     print(\"Create Corpus File\")\n",
        "#     corpus_file = open(corpus_path, \"w\")\n",
        "#     for file_path in glob.glob(\"datasets/LibriSpeech/*/*/*/*.txt\"):\n",
        "#         for line in open(file_path, \"r\").readlines():\n",
        "#             corpus_file.write(line[len(line.split()[0]) + 1:-1].lower() + \"\\n\")\n",
        "\n",
        "# # Train Tokenizer\n",
        "# print(\"Training Tokenizer\")\n",
        "# sentencepiece.SentencePieceTrainer.train(input=corpus_path, \n",
        "#                                          model_prefix=\"LibriSpeech_bpe_256\", \n",
        "#                                          vocab_size=256, \n",
        "#                                          character_coverage=1.0, \n",
        "#                                          model_type=\"bpe\", \n",
        "#                                          bos_id=-1, \n",
        "#                                          eos_id=-1, \n",
        "#                                          unk_surface=\"\")\n",
        "# print(\"Training Done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysQiLozZYHe6"
      },
      "source": [
        "Build dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zci0AHMTpDH-",
        "outputId": "a01d1496-1072-4b8a-c326-ba83fc8b5d24"
      },
      "outputs": [],
      "source": [
        "tokenizer_path = PATH + \"LibriSpeech_bpe_256.model\"\n",
        "tokenizer = sentencepiece.SentencePieceProcessor(tokenizer_path)\n",
        "\n",
        "# train_dataset = LibriSpeechDataset(\"train-clean-100\", tokenizer)\n",
        "dev_dataset = LibriSpeechDataset(\"dev-clean\", tokenizer)\n",
        "\n",
        "# train_dataset.__getitem__(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDxwsnOy0ks_"
      },
      "source": [
        "Build dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ls-O9ku_SKyd"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    # Sorting sequences by lengths\n",
        "    sorted_batch = sorted(batch, key=lambda x: x[0].shape[1], reverse=True)\n",
        "\n",
        "    # Pad data sequences\n",
        "    data = [item[0].squeeze() for item in sorted_batch]\n",
        "    data_lengths = torch.tensor([len(d) for d in data],dtype=torch.long).cuda()\n",
        "    data = torch.nn.utils.rnn.pad_sequence(data, batch_first=True, padding_value=0).cuda()\n",
        "\n",
        "    # Pad labels\n",
        "    target = [item[1] for item in sorted_batch]\n",
        "    target_lengths = torch.tensor([t.size(0) for t in target],dtype=torch.long).cuda()\n",
        "    target = torch.nn.utils.rnn.pad_sequence(target, batch_first=True, padding_value=0).cuda()\n",
        "\n",
        "    return data, target, data_lengths, target_lengths\n",
        "\n",
        "# train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
        "#                                                batch_size=4,\n",
        "#                                                shuffle=True,\n",
        "#                                                collate_fn=collate_fn,\n",
        "#                                                drop_last=True)\n",
        "\n",
        "dev_dataloader = torch.utils.data.DataLoader(dev_dataset,\n",
        "                                             batch_size=4,\n",
        "                                             shuffle=False,\n",
        "                                             collate_fn=collate_fn,\n",
        "                                             drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJLnkL3COfPU"
      },
      "source": [
        "# Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8171lGBwOeeh"
      },
      "outputs": [],
      "source": [
        "class Trainer():\n",
        "    def __init__(self,\n",
        "                 model: nn.Module,\n",
        "                 optimizer,\n",
        "                 scheduler,\n",
        "                 scaler,\n",
        "                 path,\n",
        "                 save_best_model = True,\n",
        "                 logging = True,\n",
        "                 accumulated_steps = 16):\n",
        "      \n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "        self.scaler = scaler\n",
        "        self.accumulated_steps = accumulated_steps\n",
        "\n",
        "        self.best_wer = 1\n",
        "        self.path = path\n",
        "        self.save_best_model = save_best_model\n",
        "        self.logging = logging\n",
        "\n",
        "    def train(self, \n",
        "              train_dataset,\n",
        "              dev_dataset,\n",
        "              epochs:int=20):\n",
        "        \n",
        "        print(\"Training...\")\n",
        "\n",
        "        train_loss = 0.0\n",
        "        total_loss_train = []\n",
        "        total_loss_dev = []\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(\" Epoch {:03d}\".format(epoch + 1))\n",
        "\n",
        "            epoch_loss = 0.0\n",
        "\n",
        "            # train mode on\n",
        "            self.model.train()\n",
        "\n",
        "            for step, batch in enumerate(tqdm(train_dataset)):        \n",
        "                inputs, targets, input_len, target_len = batch\n",
        "\n",
        "                # Automatic Mixed Precision Casting (model prediction + loss computing)\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    pred, pred_len = self.model.forward(inputs, input_len, targets, target_len, train=True)\n",
        "                    loss_mini  = warp_rnnt.rnnt_loss(\n",
        "                                    log_probs=torch.nn.functional.log_softmax(pred, dim=-1),\n",
        "                                    labels=targets.int(),\n",
        "                                    frames_lengths=pred_len.int(),\n",
        "                                    labels_lengths=target_len.int(),\n",
        "                                    average_frames=False,\n",
        "                                    reduction='mean',\n",
        "                                    blank=0,\n",
        "                                    gather=True)\n",
        "                    loss = loss_mini / self.accumulated_steps\n",
        "\n",
        "                # Accumulate gradients\n",
        "                scaler.scale(loss).backward()\n",
        "\n",
        "                # Update Epoch Variables\n",
        "                epoch_loss += loss_mini.detach()\n",
        "\n",
        "                if step % self.accumulated_steps == 0:\n",
        "                    # Update Parameters, Zero Gradients and Update Learning Rate\n",
        "                    self.scaler.step(self.optimizer)\n",
        "                    self.scaler.update()\n",
        "                    self.optimizer.zero_grad()\n",
        "                    self.scheduler.step()\n",
        "\n",
        "                    # Step Print\n",
        "                    print(\"mean loss {:.4f} - batch loss: {:.4f} - learning rate: {:.6f}\".format(epoch_loss / (step + 1), loss_mini, optimizer.param_groups[0]['lr']))\n",
        "\n",
        "            avg_epoch_loss = epoch_loss / len(train_dataset)\n",
        "            print('\\t[E: {:2d}] train loss = {:0.4f}'.format(epoch, avg_epoch_loss))\n",
        "\n",
        "            wer, speech_true, speech_pred, val_loss = self.evaluate(dev_dataset)\n",
        "            print('\\t[E: {:2d}] valid loss = {:0.4f}, wer = {:0.4f}, loss = {:0.4f}'.format(epoch, val_loss, wer, avg_epoch_loss))\n",
        "\n",
        "            if self.save_best_model:\n",
        "                self.save_model(wer, avg_epoch_loss, optimizer.param_groups[0]['lr'], self.path)\n",
        "\n",
        "        print(\"...Done!\")\n",
        "        return avg_epoch_loss\n",
        "\n",
        "    def evaluate(self,\n",
        "              dev_dataset):\n",
        "        \n",
        "        self.model.eval()\n",
        "\n",
        "        speech_true = []\n",
        "        speech_pred = []\n",
        "        total_wer = 0.0\n",
        "        total_loss = 0.0\n",
        "\n",
        "        # Evaluation Loop\n",
        "        for step, batch in enumerate(tqdm(dev_dataset)):\n",
        "\n",
        "            inputs, targets, input_len, target_len = batch\n",
        "\n",
        "            # Sequence Prediction\n",
        "            with torch.no_grad():\n",
        "\n",
        "                outputs_pred = greedy_search_decoding(inputs, input_len)\n",
        "\n",
        "            # Sequence Truth\n",
        "            outputs_true = tokenizer.decode(targets.tolist())\n",
        "\n",
        "            # Compute Batch wer and Update total wer\n",
        "            batch_wer = jiwer.wer(outputs_true, outputs_pred, standardize=True)\n",
        "            total_wer += batch_wer\n",
        "\n",
        "            # Update String lists\n",
        "            speech_true += outputs_true\n",
        "            speech_pred += outputs_pred\n",
        "\n",
        "            # Prediction Verbose\n",
        "            print(\"Groundtruths :\\n\", outputs_true)\n",
        "            print(\"Predictions :\\n\", outputs_pred)\n",
        "\n",
        "            # Eval Loss\n",
        "            with torch.no_grad():\n",
        "                pred, pred_len = self.model.forward(inputs, input_len, targets, target_len, train=False)\n",
        "                batch_loss = warp_rnnt.rnnt_loss(\n",
        "                                    log_probs=torch.nn.functional.log_softmax(pred, dim=-1),\n",
        "                                    labels=targets.int(),\n",
        "                                    frames_lengths=pred_len.int(),\n",
        "                                    labels_lengths=target_len.int(),\n",
        "                                    average_frames=False,\n",
        "                                    reduction='mean',\n",
        "                                    blank=0,\n",
        "                                    gather=True)\n",
        "                # batch_loss = self.loss(pred, targets, pred_len, target_len)\n",
        "                total_loss += batch_loss\n",
        "\n",
        "            # Step print\n",
        "            print(\"mean batch wer {:.2f}% - batch wer: {:.2f}% - mean loss {:.4f} - batch loss: {:.4f}\".format(100 * total_wer / (step + 1), 100 * batch_wer, total_loss / (step + 1), batch_loss))\n",
        "\n",
        "        # Compute wer\n",
        "        if total_wer / dev_dataset.__len__() > 1:\n",
        "            wer = 1\n",
        "        else:\n",
        "            wer = jiwer.wer(speech_true, speech_pred, standardize=True)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = total_loss / dev_dataset.__len__()\n",
        "\n",
        "        return wer, speech_true, speech_pred, loss\n",
        "\n",
        "    def save_model(self, wer, loss, lr, path):\n",
        "        if wer < self.best_wer:\n",
        "            torch.save(self.model.state_dict(), f\"{path}.pth\")\n",
        "            torch.save(self.scheduler.state_dict(), f\"{path}_scheduler.pth\")\n",
        "            torch.save(self.optimizer.state_dict(), f\"{path}_optimizer.pth\")\n",
        "\n",
        "        if self.logging:\n",
        "            with open(f\"{path}.tsv\", \"a\") as log:\n",
        "                log.write(f\"{lr}\\t{wer}\\t{loss}\\n\")\n",
        "               "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbHexkORYN_j"
      },
      "source": [
        "Greedy search decoding strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLPWcv6Nuc9i"
      },
      "outputs": [],
      "source": [
        "def greedy_search_decoding(x, x_len):\n",
        "\n",
        "    # Predictions String List\n",
        "    preds = []\n",
        "\n",
        "    # Forward Encoder (B, Taud) -> (B, T, Denc)\n",
        "    f, f_len = model.encoder(x, x_len, train=False)\n",
        "\n",
        "    # Batch loop\n",
        "    for b in range(x.size(0)): # One sample at a time for now, not batch optimized\n",
        "\n",
        "        # Init y and hidden state\n",
        "        y = x.new_zeros(1, 1, dtype=torch.long)\n",
        "        hidden = None\n",
        "\n",
        "        enc_step = 0\n",
        "        consec_dec_step = 0\n",
        "\n",
        "        # Decoder loop\n",
        "        while enc_step < f_len[b]:\n",
        "\n",
        "            # Forward Decoder (1, 1) -> (1, 1, Ddec)\n",
        "            g, hidden = model.decoder(y[:, -1:], hidden_states=hidden)\n",
        "            \n",
        "            # Joint Network loop\n",
        "            while enc_step < f_len[b]:\n",
        "\n",
        "                # Forward Joint Network (1, 1, Denc) and (1, 1, Ddec) -> (1, V)\n",
        "                logits = model.joint(f[b:b+1, enc_step], g[:, 0])\n",
        "\n",
        "                # Token Prediction\n",
        "                pred = logits.softmax(dim=-1).log().argmax(dim=-1) # (1)\n",
        "\n",
        "                # Null token or max_consec_dec_step\n",
        "                if pred == 0 or consec_dec_step == 5:\n",
        "                    consec_dec_step = 0\n",
        "                    enc_step += 1\n",
        "                # Token\n",
        "                else:\n",
        "                    consec_dec_step += 1\n",
        "                    y = torch.cat([y, pred.unsqueeze(0)], axis=-1)\n",
        "                    break\n",
        "\n",
        "        # Decode Label Sequence\n",
        "        pred = tokenizer.decode(y[:, 1:].tolist())\n",
        "        preds += pred\n",
        "\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtOTtvgpu1ut"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7ModXBbvLEj"
      },
      "source": [
        "### Activation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWbSCjTru_J7"
      },
      "outputs": [],
      "source": [
        "class Swish(nn.Module):\n",
        "    r\"\"\"\n",
        "    In the ContextNet paper, the swish activation function works consistently better than ReLU.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(Swish, self).__init__()\n",
        "\n",
        "    def forward(self, inputs: Tensor) -> Tensor:\n",
        "        return inputs * inputs.sigmoid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3cgY2buvrbL"
      },
      "source": [
        "### Squeeze-and-excitation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMvvBQ0Ovud8"
      },
      "outputs": [],
      "source": [
        "class SELayer(nn.Module):\n",
        "    r\"\"\"\n",
        "    Squeeze-and-excitation module.\n",
        "    Args:\n",
        "        dim (int): Dimension to be used for two fully connected (FC) layers\n",
        "    Inputs: inputs, input_lengths\n",
        "        - **inputs**: The output of the last convolution layer. `FloatTensor` of size\n",
        "            ``(batch, dimension, seq_length)``\n",
        "        - **input_lengths**: The length of input tensor. ``(batch)``\n",
        "    Returns: output\n",
        "        - **output**: Output of SELayer `FloatTensor` of size\n",
        "            ``(batch, dimension, seq_length)``\n",
        "    \"\"\"\n",
        "    def __init__(self, dim: int) -> None:\n",
        "        super(SELayer, self).__init__()\n",
        "        assert dim % 8 == 0, 'Dimension should be divisible by 8.'\n",
        "\n",
        "        self.dim = dim\n",
        "        self.sequential = nn.Sequential(\n",
        "            nn.Linear(dim, dim // 8),\n",
        "            Swish(),\n",
        "            nn.Linear(dim // 8, dim),\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            inputs: Tensor,\n",
        "            input_lengths: Tensor,\n",
        "    ) -> Tuple[Tensor, Tensor]:\n",
        "        \"\"\"\n",
        "        Forward propagate a `inputs` for SE Layer.\n",
        "        Args:\n",
        "            **inputs** (torch.FloatTensor): The output of the last convolution layer. `FloatTensor` of size\n",
        "                ``(batch, dimension, seq_length)``\n",
        "            **input_lengths** (torch.LongTensor): The length of input tensor. ``(batch)``\n",
        "        Returns:\n",
        "            **output** (torch.FloatTensor): Output of SELayer `FloatTensor` of size\n",
        "                ``(batch, dimension, seq_length)``\n",
        "        \"\"\"\n",
        "        residual = inputs\n",
        "        seq_lengths = inputs.size(2)\n",
        "\n",
        "        inputs = inputs.sum(dim=2) / input_lengths.unsqueeze(1)\n",
        "        output = self.sequential(inputs)\n",
        "\n",
        "        output = output.sigmoid().unsqueeze(2)\n",
        "        output = output.repeat(1, 1, seq_lengths)\n",
        "\n",
        "        return output * residual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyjHRnAR1mNX"
      },
      "source": [
        "### Convolutional layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGVhRZXH1k7y"
      },
      "outputs": [],
      "source": [
        "class ConvLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    When the stride is 1, it pads the input so the output has the shape as the input.\n",
        "    And when the stride is 2, it does not pad the input.\n",
        "    Args:\n",
        "        in_channels (int): Input channel in convolutional layer\n",
        "        out_channels (int): Output channel in convolutional layer\n",
        "        kernel_size (int, optional): Value of convolution kernel size (default : 5)\n",
        "        stride(int, optional): Value of stride (default : 1)\n",
        "        padding (int, optional): Value of padding (default: 0)\n",
        "        activation (bool, optional): Flag indication use activation function or not (default : True)\n",
        "        groups(int, optional): Value of groups (default : 1)\n",
        "        bias (bool, optional): Flag indication use bias or not (default : True)\n",
        "    Inputs: inputs, input_lengths\n",
        "        - **inputs**: Input of convolution layer `FloatTensor` of size ``(batch, dimension, seq_length)``\n",
        "        - **input_lengths**: The length of input tensor. ``(batch)``\n",
        "    Returns: output, output_lengths\n",
        "        - **output**: Output of convolution layer `FloatTensor` of size\n",
        "                ``(batch, dimension, seq_length)``\n",
        "        - **output_lengths**: The length of output tensor. ``(batch)``\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels: int,\n",
        "            out_channels: int,\n",
        "            kernel_size: int = 5,\n",
        "            stride: int = 1,\n",
        "            padding: int = 0,\n",
        "            activation: bool = True,\n",
        "            groups: int = 1,\n",
        "            bias: bool = True,\n",
        "    ):\n",
        "        super(ConvLayer, self).__init__()\n",
        "        assert kernel_size == 5, \"The convolution layer in the ContextNet model has 5 kernels.\"\n",
        "\n",
        "        if stride == 1:\n",
        "            self.conv = nn.Conv1d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=kernel_size,\n",
        "                stride=stride,\n",
        "                dilation=1,\n",
        "                padding=(kernel_size - 1) // 2,\n",
        "                groups=groups,\n",
        "                bias=bias,\n",
        "            )\n",
        "        elif stride == 2:\n",
        "            self.conv = nn.Conv1d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=kernel_size,\n",
        "                stride=stride,\n",
        "                dilation=1,\n",
        "                padding=padding,\n",
        "                groups=groups,\n",
        "                bias=bias,\n",
        "            )\n",
        "\n",
        "        self.batch_norm = nn.BatchNorm1d(num_features=out_channels)\n",
        "        self.activation = activation\n",
        "\n",
        "        if self.activation:\n",
        "            self.swish = Swish()\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            inputs: Tensor,\n",
        "            input_lengths: Tensor,\n",
        "    ) -> Tuple[Tensor, Tensor]:\n",
        "        \"\"\"\n",
        "        Forward propagate a `inputs` for convolution layer.\n",
        "        Args:\n",
        "            **inputs** (torch.FloatTensor): Input of convolution layer `FloatTensor` of size\n",
        "                ``(batch, dimension, seq_length)``\n",
        "            **input_lengths** (torch.LongTensor): The length of input tensor. ``(batch)``\n",
        "        Returns:\n",
        "            **output** (torch.FloatTensor): Output of convolution layer `FloatTensor` of size\n",
        "                ``(batch, dimension, seq_length)``\n",
        "            **output_lengths** (torch.LongTensor): The length of output tensor. ``(batch)``\n",
        "        \"\"\"\n",
        "        outputs, output_lengths = self.conv(inputs), self._get_sequence_lengths(input_lengths)\n",
        "        outputs = self.batch_norm(outputs)\n",
        "\n",
        "        if self.activation:\n",
        "            outputs = self.swish(outputs)\n",
        "\n",
        "        return outputs, output_lengths\n",
        "\n",
        "    def _get_sequence_lengths(self, seq_lengths):\n",
        "        # return (\n",
        "        #         (seq_lengths + 2 * self.conv.padding[0]\n",
        "        #          - self.conv.dilation[0] * (self.conv.kernel_size[0] - 1) - 1) // self.conv.stride[0] + 1\n",
        "        # )\n",
        "        a = seq_lengths + 2 * self.conv.padding[0] - self.conv.dilation[0] * (self.conv.kernel_size[0] - 1) - 1\n",
        "        b = self.conv.stride[0]\n",
        "        return torch.div(a, b, rounding_mode='floor') + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4CF8k257cL3"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGri2sIQ7eHv"
      },
      "outputs": [],
      "source": [
        "class AudioPreprocessing(nn.Module):\n",
        "\n",
        "    \"\"\"Audio Preprocessing\n",
        "    Computes mel-scale log filter banks spectrogram\n",
        "    Args:\n",
        "        sample_rate: Audio sample rate\n",
        "        n_fft: FFT frame size, creates n_fft // 2 + 1 frequency bins.\n",
        "        win_length_ms: FFT window length in ms, must be <= n_fft\n",
        "        hop_length_ms: length of hop between FFT windows in ms\n",
        "        n_mels: number of mel filter banks\n",
        "        normalize: whether to normalize mel spectrograms outputs\n",
        "        mean: training mean\n",
        "        std: training std\n",
        "    Shape:\n",
        "        Input: (batch_size, audio_len)\n",
        "        Output: (batch_size, n_mels, audio_len // hop_length + 1)\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, \n",
        "                 sample_rate, \n",
        "                 n_fft, \n",
        "                 win_length_ms, \n",
        "                 hop_length_ms, \n",
        "                 n_mels, \n",
        "                 normalize, \n",
        "                 mean, \n",
        "                 std):\n",
        "        super(AudioPreprocessing, self).__init__()\n",
        "        self.win_length = int(sample_rate * win_length_ms) // 1000\n",
        "        self.hop_length = int(sample_rate * hop_length_ms) // 1000\n",
        "        self.Spectrogram = torchaudio.transforms.Spectrogram(n_fft, self.win_length, self.hop_length)\n",
        "        self.MelScale = torchaudio.transforms.MelScale(n_mels, sample_rate, f_min=0, f_max=8000, n_stft=n_fft // 2 + 1)\n",
        "        self.normalize = normalize\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def forward(self, x, x_len):\n",
        "\n",
        "        # Short Time Fourier Transform (B, T) -> (B, n_fft // 2 + 1, T // hop_length + 1)\n",
        "        x = self.Spectrogram(x)\n",
        "\n",
        "        # Mel Scale (B, n_fft // 2 + 1, T // hop_length + 1) -> (B, n_mels, T // hop_length + 1)\n",
        "        x = self.MelScale(x)\n",
        "        \n",
        "        # Energy log, autocast disabled to prevent float16 overflow\n",
        "        x = (x.float() + 1e-9).log().type(x.dtype)\n",
        "\n",
        "        # Compute Sequence lengths \n",
        "        if x_len is not None:\n",
        "            x_len = torch.div(x_len, self.hop_length, rounding_mode='floor') + 1\n",
        "\n",
        "        # Normalize\n",
        "        if self.normalize:\n",
        "            x = (x - self.mean) / self.std\n",
        "\n",
        "        x = x.transpose(1,2)\n",
        "        \n",
        "        return x, x_len\n",
        "\n",
        "class SpecAugment(nn.Module):\n",
        "\n",
        "    \"\"\"Spectrogram Augmentation\n",
        "    Args:\n",
        "        spec_augment: whether to apply spec augment\n",
        "        mF: number of frequency masks\n",
        "        F: maximum frequency mask size\n",
        "        mT: number of time masks\n",
        "        pS: adaptive maximum time mask size in %\n",
        "    References:\n",
        "        SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition, Park et al.\n",
        "        https://arxiv.org/abs/1904.08779\n",
        "        SpecAugment on Large Scale Datasets, Park et al.\n",
        "        https://arxiv.org/abs/1912.05533\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, spec_augment, mF, F, mT, pS):\n",
        "        super(SpecAugment, self).__init__()\n",
        "        self.spec_augment = spec_augment\n",
        "        self.mF = mF\n",
        "        self.F = F\n",
        "        self.mT = mT\n",
        "        self.pS = pS\n",
        "\n",
        "    def forward(self, x, x_len):\n",
        "\n",
        "        # Spec Augment\n",
        "        if self.spec_augment:\n",
        "        \n",
        "            # Frequency Masking\n",
        "            for _ in range(self.mF):\n",
        "                x = torchaudio.transforms.FrequencyMasking(freq_mask_param=self.F, iid_masks=False).forward(x)\n",
        "\n",
        "            # Time Masking\n",
        "            for b in range(x.size(0)):\n",
        "                T = int(self.pS * x_len[b])\n",
        "                for _ in range(self.mT):\n",
        "                    x[b, :, :x_len[b]] = torchaudio.transforms.TimeMasking(time_mask_param=T).forward(x[b, :, :x_len[b]])\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDp8hxwKvd8B"
      },
      "source": [
        "### Convolution Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JY1URLqFvk4l"
      },
      "outputs": [],
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Convolution block contains a number of convolutions, each followed by batch normalization and activation.\n",
        "    Squeeze-and-excitation (SE) block operates on the output of the last convolution layer.\n",
        "    Skip connection with projection is applied on the output of the squeeze-and-excitation block.\n",
        "    Args:\n",
        "        in_channels (int): Input channel in convolutional layer\n",
        "        out_channels (int): Output channel in convolutional layer\n",
        "        num_layers (int, optional): The number of convolutional layers (default : 5)\n",
        "        kernel_size (int, optional): Value of convolution kernel size (default : 5)\n",
        "        stride(int, optional): Value of stride (default : 1)\n",
        "        padding (int, optional): Value of padding (default: 0)\n",
        "        residual (bool, optional): Flag indication residual or not (default : True)\n",
        "    Inputs: inputs, input_lengths\n",
        "        - **inputs**: Input of convolution block `FloatTensor` of size ``(batch, dimension, seq_length)``\n",
        "        - **input_lengths**: The length of input tensor. ``(batch)``\n",
        "    Returns: output, output_lengths\n",
        "        - **output**: Output of convolution block `FloatTensor` of size\n",
        "                ``(batch, dimension, seq_length)``\n",
        "        - **output_lengths**: The length of output tensor. ``(batch)``\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels: int,\n",
        "            out_channels: int,\n",
        "            num_layers: int = 5,\n",
        "            kernel_size: int = 5,\n",
        "            stride: int = 1,\n",
        "            padding: int = 0,\n",
        "            residual: bool = True,\n",
        "    ) -> None:\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.swish = Swish()\n",
        "        self.se_layer = SELayer(out_channels)\n",
        "        self.residual = None\n",
        "\n",
        "        if residual:\n",
        "            self.residual = ConvLayer(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=kernel_size,\n",
        "                stride=stride,\n",
        "                padding=padding,\n",
        "                activation=False,\n",
        "            )\n",
        "\n",
        "        if self.num_layers == 1:\n",
        "            self.conv_layers = ConvLayer(\n",
        "                        in_channels=in_channels,\n",
        "                        out_channels=out_channels,\n",
        "                        kernel_size=kernel_size,\n",
        "                        stride=stride,\n",
        "                        padding=padding,\n",
        "                    )\n",
        "\n",
        "        else:\n",
        "            stride_list = [1 for _ in range(num_layers - 1)] + [stride]\n",
        "            in_channel_list = [in_channels] + [out_channels for _ in range(num_layers - 1)]\n",
        "\n",
        "            self.conv_layers = nn.ModuleList(list())\n",
        "            for in_channels, stride in zip(in_channel_list, stride_list):\n",
        "                self.conv_layers.append(\n",
        "                    ConvLayer(\n",
        "                        in_channels=in_channels,\n",
        "                        out_channels=out_channels,\n",
        "                        kernel_size=kernel_size,\n",
        "                        stride=stride,\n",
        "                        padding=padding,\n",
        "                    )\n",
        "                )\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            inputs: Tensor,\n",
        "            input_lengths: Tensor,\n",
        "    ) -> Tuple[Tensor, Tensor]:\n",
        "        \"\"\"\n",
        "        Forward propagate a `inputs` for convolution block.\n",
        "        Args:\n",
        "            **inputs** (torch.FloatTensor): Input of convolution block `FloatTensor` of size\n",
        "                ``(batch, dimension, seq_length)``\n",
        "            **input_lengths** (torch.LongTensor): The length of input tensor. ``(batch)``\n",
        "        Returns:\n",
        "            **output** (torch.FloatTensor): Output of convolution block `FloatTensor` of size\n",
        "                ``(batch, dimension, seq_length)``\n",
        "            **output_lengths** (torch.LongTensor): The length of output tensor. ``(batch)``\n",
        "        \"\"\"\n",
        "        output = inputs\n",
        "        output_lengths = input_lengths\n",
        "\n",
        "        if self.num_layers == 1:\n",
        "            output, output_lengths = self.conv_layers(output, output_lengths)\n",
        "        else:\n",
        "            for conv_layer in self.conv_layers:\n",
        "                output, output_lengths = conv_layer(output, output_lengths)\n",
        "\n",
        "        output = self.se_layer(output, output_lengths)\n",
        "\n",
        "        if self.residual is not None:\n",
        "            residual, _ = self.residual(inputs, input_lengths)\n",
        "            output += residual\n",
        "\n",
        "        return self.swish(output), output_lengths\n",
        "\n",
        "    @staticmethod\n",
        "    def make_conv_blocks(\n",
        "            input_dim: int = 80,\n",
        "            num_layers: int = 5,\n",
        "            kernel_size: int = 5,\n",
        "            num_channels: int = 256,\n",
        "            output_dim: int = 640,\n",
        "    ) -> nn.ModuleList:\n",
        "        r\"\"\"\n",
        "        Create 23 convolution blocks.\n",
        "        Args:\n",
        "            input_dim (int, optional): Dimension of input vector (default : 80)\n",
        "            num_layers (int, optional): The number of convolutional layers (default : 5)\n",
        "            kernel_size (int, optional): Value of convolution kernel size (default : 5)\n",
        "            num_channels (int, optional): The number of channels in the convolution filter (default: 256)\n",
        "            output_dim (int, optional): Dimension of encoder output vector (default: 640)\n",
        "        Returns:\n",
        "            **conv_blocks** (nn.ModuleList): ModuleList with 23 convolution blocks\n",
        "        \"\"\"\n",
        "        conv_blocks = nn.ModuleList()\n",
        "\n",
        "        # C0 : 1 conv layer, init_dim output channels, stride 1, no residual\n",
        "        conv_blocks.append(ConvBlock(input_dim, \n",
        "                                     num_channels, \n",
        "                                     1, \n",
        "                                     kernel_size, \n",
        "                                     1, \n",
        "                                     0, \n",
        "                                     False))\n",
        "\n",
        "        # C1-2 : 5 conv layers, init_dim output channels, stride 1\n",
        "        for _ in range(1, 2 + 1):\n",
        "            conv_blocks.append(ConvBlock(num_channels, \n",
        "                                         num_channels, \n",
        "                                         num_layers, \n",
        "                                         kernel_size, \n",
        "                                         1, \n",
        "                                         0, \n",
        "                                         True))\n",
        "\n",
        "        # C3 : 5 conv layer, init_dim output channels, stride 2\n",
        "        conv_blocks.append(ConvBlock(num_channels, \n",
        "                                     num_channels, \n",
        "                                     num_layers, \n",
        "                                     kernel_size, \n",
        "                                     2, \n",
        "                                     0, \n",
        "                                     True))\n",
        "\n",
        "        # C4-6 : 5 conv layers, init_dim output channels, stride 1\n",
        "        for _ in range(4, 6 + 1):\n",
        "            conv_blocks.append(ConvBlock(num_channels, \n",
        "                                         num_channels, \n",
        "                                         num_layers, \n",
        "                                         kernel_size, \n",
        "                                         1, \n",
        "                                         0, \n",
        "                                         True))\n",
        "\n",
        "        # C7 : 5 conv layers, init_dim output channels, stride 2\n",
        "        conv_blocks.append(ConvBlock(num_channels, \n",
        "                                     num_channels, \n",
        "                                     num_layers, \n",
        "                                     kernel_size, \n",
        "                                     2, \n",
        "                                     0, \n",
        "                                     True))\n",
        "\n",
        "        # C8-10 : 5 conv layers, init_dim output channels, stride 1\n",
        "        for _ in range(8, 10 + 1):\n",
        "            conv_blocks.append(ConvBlock(num_channels, \n",
        "                                         num_channels, \n",
        "                                         num_layers, \n",
        "                                         kernel_size, \n",
        "                                         1, \n",
        "                                         0, \n",
        "                                         True))\n",
        "\n",
        "        # C11-13 : 5 conv layers, middle_dim output channels, stride 1\n",
        "        conv_blocks.append(ConvBlock(num_channels, \n",
        "                                     num_channels << 1, \n",
        "                                     num_layers, \n",
        "                                     kernel_size, \n",
        "                                     1, \n",
        "                                     0, \n",
        "                                     True))\n",
        "        for _ in range(12, 13 + 1):\n",
        "            conv_blocks.append(ConvBlock(num_channels << 1, \n",
        "                                         num_channels << 1, \n",
        "                                         num_layers, \n",
        "                                         kernel_size, \n",
        "                                         1, 0, True))\n",
        "\n",
        "        # C14 : 5 conv layers, middle_dim output channels, stride 2\n",
        "        conv_blocks.append(ConvBlock(num_channels << 1, \n",
        "                                     num_channels << 1, \n",
        "                                     num_layers, \n",
        "                                     kernel_size, \n",
        "                                     2, 0, True))\n",
        "\n",
        "        # C15-21 : 5 conv layers, middle_dim output channels, stride 1\n",
        "        for i in range(15, 21 + 1):\n",
        "            conv_blocks.append(ConvBlock(num_channels << 1, \n",
        "                                         num_channels << 1, \n",
        "                                         num_layers, \n",
        "                                         kernel_size, \n",
        "                                         1, 0, True))\n",
        "\n",
        "        # C22 : 1 conv layer, final_dim output channels, stride 1, no residual\n",
        "        conv_blocks.append(ConvBlock(num_channels << 1, \n",
        "                                     output_dim, \n",
        "                                     1, \n",
        "                                     kernel_size, \n",
        "                                     1, 0, False))\n",
        "\n",
        "        return conv_blocks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH3j4hg5vVo7"
      },
      "source": [
        "## ContextNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-42JvCe6uH7"
      },
      "source": [
        "### Audio Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aulez2Z-vbQY"
      },
      "outputs": [],
      "source": [
        "class AudioEncoder(nn.Module):\n",
        "    r\"\"\"\n",
        "    Audio encoder goes through 23 convolution blocks to convert to higher feature values.\n",
        "    Args:\n",
        "        input_dim (int, optional): Dimension of input vector (default : 80)\n",
        "        num_layers (int, optional): The number of convolution layers (default : 5)\n",
        "        kernel_size (int, optional): Value of convolution kernel size (default : 5)\n",
        "        num_channels (int, optional): The number of channels in the convolution filter (default: 256)\n",
        "        output_dim (int, optional): Dimension of encoder output vector (default: 640)\n",
        "    Inputs: inputs, input_lengths\n",
        "        - **inputs**: Parsed audio of batch size number `FloatTensor` of size ``(batch, seq_length, dimension)``\n",
        "        - **input_lengths**: Tensor representing the sequence length of the input ``(batch)``\n",
        "    Returns: output, output_lengths\n",
        "        - **output**: Tensor of encoder output `FloatTensor` of size\n",
        "                ``(batch, seq_length, dimension)``\n",
        "        - **output_lengths**: Tensor representing the length of the encoder output ``(batch)``\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            preprocessing_params,\n",
        "            input_dim: int = 80,\n",
        "            num_layers: int = 5,\n",
        "            kernel_size: int = 5,\n",
        "            num_channels: int = 256,\n",
        "            output_dim: int = 640,\n",
        "    ) -> None:\n",
        "        super(AudioEncoder, self).__init__()\n",
        "        self.preprocessing = AudioPreprocessing(preprocessing_params[\"sample_rate\"], \n",
        "                                                preprocessing_params[\"n_fft\"],\n",
        "                                                preprocessing_params[\"win_length_ms\"], \n",
        "                                                preprocessing_params[\"hop_length_ms\"],\n",
        "                                                preprocessing_params[\"n_mels\"], \n",
        "                                                preprocessing_params[\"normalize\"], \n",
        "                                                preprocessing_params[\"mean\"], \n",
        "                                                preprocessing_params[\"std\"])\n",
        "        \n",
        "        self.augment = SpecAugment(preprocessing_params[\"spec_augment\"], \n",
        "                                   preprocessing_params[\"mF\"], \n",
        "                                   preprocessing_params[\"F\"], \n",
        "                                   preprocessing_params[\"mT\"], \n",
        "                                   preprocessing_params[\"pS\"])\n",
        "        \n",
        "        self.blocks = ConvBlock.make_conv_blocks(input_dim, \n",
        "                                                num_layers, \n",
        "                                                kernel_size, \n",
        "                                                num_channels, \n",
        "                                                output_dim)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            inputs: Tensor,\n",
        "            input_lengths: Tensor,\n",
        "            train=True\n",
        "    ) -> Tuple[Tensor, Tensor]:\n",
        "        r\"\"\"\n",
        "        Forward propagate a `inputs` for audio encoder.\n",
        "        Args:\n",
        "            **inputs** (torch.FloatTensor): Parsed audio of batch size number `FloatTensor` of size\n",
        "                ``(batch, seq_length, dimension)``\n",
        "            **input_lengths** (torch.LongTensor): Tensor representing the sequence length of the input\n",
        "                `LongTensor` of size ``(batch)``\n",
        "        Returns:\n",
        "            **output** (torch.FloatTensor): Tensor of encoder output `FloatTensor` of size\n",
        "                ``(batch, seq_length, dimension)``\n",
        "            **output_lengths** (torch.LongTensor): Tensor representing the length of the encoder output\n",
        "                `LongTensor` of size ``(batch)``\n",
        "        \"\"\"\n",
        "        inputs, input_lengths = self.preprocessing(inputs, input_lengths)\n",
        "        if train:\n",
        "            inputs = self.augment(inputs, input_lengths)\n",
        "        output = inputs.transpose(1, 2)\n",
        "        output_lengths = input_lengths\n",
        "\n",
        "        for block in self.blocks:\n",
        "            output, output_lengths = block(output, output_lengths)\n",
        "\n",
        "        return output.transpose(1, 2), output_lengths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsrDnq1o6yGH"
      },
      "source": [
        "### Label Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pb97KJRYvXe8"
      },
      "outputs": [],
      "source": [
        "class LabelEncoder(nn.Module):\n",
        "    r\"\"\"\n",
        "    Label encoder goes through a one-layered lstm model to convert to higher feature values.\n",
        "    Args:\n",
        "        num_vocabs (int): The number of vocabulary\n",
        "        output_dim (int, optional): Dimension of decoder output vector (default: 640)\n",
        "        hidden_dim (int, optional): The number of features in the decoder hidden state (default : 2048)\n",
        "        num_layers (int, optional): The number of rnn layers (default : 1)\n",
        "        dropout (float, optional): Dropout probability of decoder (default: 0.3)\n",
        "        rnn_type (str, optional): Type of RNN cell (default: lstm)\n",
        "        sos_id (int, optional): Index of the start of sentence (default: 1)\n",
        "    Inputs: inputs, input_lengths, hidden_states\n",
        "        - **inputs**: Tensor representing the target `LongTensor` of size ``(batch, seq_length)``\n",
        "        - **input_lengths**: Tensor representing the target length `LongTensor` of size ``(batch)``\n",
        "        - **hidden_states**: A previous hidden state of decoder `FloatTensor` of size\n",
        "            ``(batch, seq_length, dimension)``\n",
        "    Returns: outputs, hidden_states\n",
        "        - **outputs**: A output sequence of decoder `FloatTensor` of size\n",
        "                ``(batch, seq_length, dimension)``\n",
        "        - **hidden_states**: A hidden state of decoder. `FloatTensor` of size\n",
        "                ``(batch, seq_length, dimension)``\n",
        "    \"\"\"\n",
        "    supported_rnns = {\n",
        "        'rnn': nn.RNN,\n",
        "        'lstm': nn.LSTM,\n",
        "        'gru': nn.GRU\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            num_vocabs: int = 256,\n",
        "            output_dim: int = 640,\n",
        "            hidden_dim: int = 2048,\n",
        "            num_layers: int = 1,\n",
        "            dropout: float = 0.3,\n",
        "            rnn_type: str = 'lstm',\n",
        "            sos_id: int = 1,\n",
        "    ) -> None:\n",
        "        super(LabelEncoder, self).__init__()\n",
        "        self.sos_id = sos_id\n",
        "        self.embedding = nn.Embedding(num_vocabs, hidden_dim)\n",
        "        self.rnn = self.supported_rnns[rnn_type](hidden_dim, hidden_dim, num_layers, True, True, dropout, False)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            inputs: Tensor,\n",
        "            input_lengths: Tensor = None,\n",
        "            hidden_states: Tensor = None,\n",
        "    ) -> Tuple[Tensor, Tensor]:\n",
        "        r\"\"\"\n",
        "        Forward propagate a `inputs` for label encoder.\n",
        "        Args:\n",
        "            **inputs** (torch.LongTensor): Tensor representing the target `LongTensor` of size\n",
        "                ``(batch, seq_length)``\n",
        "            **input_lengths** (torch.LongTensor): Tensor representing the target length `LongTensor` of size\n",
        "                ``(batch)``\n",
        "            **hidden_states** (torch.FloatTensor): A previous hidden state of decoder. `FloatTensor` of size\n",
        "                ``(batch, seq_length, dimension)``\n",
        "        Returns:\n",
        "            **outputs** (torch.FloatTensor): A output sequence of decoder `FloatTensor` of size\n",
        "                ``(batch, seq_length, dimension)``\n",
        "            **hidden_states** (torch.FloatTensor): A hidden state of decoder. `FloatTensor` of size\n",
        "                ``(batch, seq_length, dimension)``\n",
        "        \"\"\"\n",
        "\n",
        "        embedded = self.embedding(inputs)\n",
        "\n",
        "        if input_lengths is not None:\n",
        "            embedded = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "            rnn_output, hidden = self.rnn(embedded, hidden_states)\n",
        "            rnn_output, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_output, batch_first=True)\n",
        "\n",
        "        else:\n",
        "            rnn_output, hidden_states = self.rnn(embedded, hidden_states)\n",
        "\n",
        "        output = self.fc(rnn_output)\n",
        "\n",
        "        return output, hidden_states"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7fTrO0V60bZ"
      },
      "source": [
        "### ContextNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BaDP1VHBu1Hy"
      },
      "outputs": [],
      "source": [
        "class ContextNet(nn.Module):\n",
        "    \"\"\"\n",
        "    ContextNet has CNN-RNN-transducer architecture and features a fully convolutional encoder that incorporates\n",
        "    global context information into convolution layers by adding squeeze-and-excitation modules.\n",
        "    Also, ContextNet supports three size models: small, medium, and large.\n",
        "    ContextNet uses the global parameter alpha to control the scaling of the model\n",
        "    by changing the number of channels in the convolution filter.\n",
        "    Args:\n",
        "        num_vocabs (int): The number of vocabulary\n",
        "        model_size (str, optional): Size of the model['small', 'medium', 'large'] (default : 'medium')\n",
        "        input_dim (int, optional): Dimension of input vector (default : 80)\n",
        "        encoder_num_layers (int, optional): The number of convolutional layers (default : 5)\n",
        "        decoder_num_layers (int, optional): The number of rnn layers (default : 1)\n",
        "        kernel_size (int, optional): Value of convolution kernel size (default : 5)\n",
        "        num_channels (int, optional): The number of channels in the convolution filter (default: 256)\n",
        "        hidden_dim (int, optional): The number of features in the decoder hidden state (default : 2048)\n",
        "        encoder_output_dim (int, optional): Dimension of encoder output vector (default: 640)\n",
        "        decoder_output_dim (int, optional): Dimension of decoder output vector (default: 640)\n",
        "        dropout (float, optional): Dropout probability of decoder (default: 0.3)\n",
        "        rnn_type (str, optional): Type of RNN cell (default: lstm)\n",
        "        sos_id (int, optional): Index of the start of sentence (default: 1)\n",
        "    Inputs: inputs, input_lengths, targets, target_lengths\n",
        "        - **inputs** (torch.FloatTensor): Parsed audio of batch size number `FloatTensor` of size\n",
        "            ``(batch, seq_length, dimension)``\n",
        "        - **input_lengths** (torch.LongTensor): Tensor representing the sequence length of the input `LongTensor` of size\n",
        "            ``(batch)``\n",
        "        - **targets** (torch.LongTensor): Tensor representing the target `LongTensor` of size\n",
        "            ``(batch, seq_length)``\n",
        "        - **target_lengths** (torch.LongTensor): Tensor representing the target length `LongTensor` of size\n",
        "            ``(batch)``\n",
        "    Returns: output\n",
        "        - **output** (torch.FloatTensor): Result of model predictions\n",
        "    \"\"\"\n",
        "    supported_models = {\n",
        "        'small': 0.5,\n",
        "        'medium': 1,\n",
        "        'large': 2,\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            preprocessing_params,\n",
        "            num_vocabs: int = 1000,\n",
        "            model_size: str = 'small',\n",
        "            input_dim: int = 80,\n",
        "            encoder_num_layers: int = 5,\n",
        "            decoder_num_layers: int = 1,\n",
        "            kernel_size: int = 5,\n",
        "            num_channels: int = 256,\n",
        "            hidden_dim: int = 2048,\n",
        "            encoder_output_dim: int = 640,\n",
        "            decoder_output_dim: int = 640,\n",
        "            dropout: float = 0.3,\n",
        "            rnn_type: str = 'lstm',\n",
        "            sos_id: int = 1\n",
        "        ) -> None:\n",
        "        super(ContextNet, self).__init__()\n",
        "        assert model_size in ('small', 'medium', 'large'), f'{model_size} is not supported.'\n",
        "\n",
        "        alpha = self.supported_models[model_size]\n",
        "\n",
        "        num_channels = int(num_channels * alpha)\n",
        "        encoder_output_dim = int(encoder_output_dim * alpha)\n",
        "\n",
        "        self.encoder = AudioEncoder(\n",
        "            input_dim=input_dim,\n",
        "            num_layers=encoder_num_layers,\n",
        "            kernel_size=kernel_size,\n",
        "            num_channels=num_channels,\n",
        "            output_dim=encoder_output_dim,\n",
        "            preprocessing_params=preprocessing_params,\n",
        "        )\n",
        "        self.decoder = LabelEncoder(\n",
        "            num_vocabs=num_vocabs,\n",
        "            output_dim=decoder_output_dim,\n",
        "            hidden_dim=hidden_dim,\n",
        "            num_layers=decoder_num_layers,\n",
        "            dropout=dropout,\n",
        "            rnn_type=rnn_type,\n",
        "            sos_id=sos_id,\n",
        "        )\n",
        "        self.joint = JointNet(num_vocabs, encoder_output_dim + decoder_output_dim)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            inputs: Tensor,\n",
        "            input_lengths: Tensor,\n",
        "            targets: Tensor,\n",
        "            target_lengths: Tensor,\n",
        "            train=True\n",
        "    ) -> Tensor:\n",
        "        r\"\"\"\n",
        "        Forward propagate a `inputs` for label encoder.\n",
        "        Args:\n",
        "            **inputs** (torch.FloatTensor): Parsed audio of batch size number `FloatTensor` of size\n",
        "                ``(batch, seq_length, dimension)``\n",
        "            **input_lengths** (torch.LongTensor): Tensor representing the sequence length of the input\n",
        "                `LongTensor` of size ``(batch)``\n",
        "            **targets** (torch.LongTensor): Tensor representing the target `LongTensor` of size\n",
        "                ``(batch, seq_length)``\n",
        "            **target_lengths** (torch.LongTensor): Tensor representing the target length `LongTensor` of size\n",
        "                ``(batch)``\n",
        "        Returns:\n",
        "            **output** (torch.FloatTensor): Result of model predictions\n",
        "        \"\"\"\n",
        "        encoder_output, encoder_output_lengths = self.encoder(inputs, input_lengths, train=train)\n",
        "\n",
        "        self.decoder.rnn.flatten_parameters()\n",
        "        targets = torch.nn.functional.pad(targets, pad=(1, 0, 0, 0), value=0)\n",
        "        target_lengths = target_lengths + 1\n",
        "        decoder_output, _ = self.decoder(targets, target_lengths)\n",
        "\n",
        "        output = self.joint(encoder_output, decoder_output)\n",
        "\n",
        "        return output, encoder_output_lengths\n",
        "\n",
        "class JointNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Joint `encoder_output` and `decoder_output`.\n",
        "    Args:\n",
        "        num_vocabs (int): The number of vocabulary\n",
        "        output_dim (int): Encoder output dimension plus Decoder output dimension\n",
        "    Inputs: encoder_output, decoder_output\n",
        "        - **encoder_output** (torch.FloatTensor): A output sequence of encoder `FloatTensor` of size\n",
        "            ``(batch, seq_length, dimension)``\n",
        "        - **decoder_output** (torch.FloatTensor): A output sequence of decoder `FloatTensor` of size\n",
        "            ``(batch, seq_length, dimension)``\n",
        "    Returns: output\n",
        "        - **output** (torch.FloatTensor): Result of joint `encoder_output` and `decoder_output`\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            num_vocabs: int,\n",
        "            output_dim: int,\n",
        "    ) -> None:\n",
        "        super(JointNet, self).__init__()\n",
        "        self.fc = nn.Linear(output_dim, num_vocabs)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            encoder_output: Tensor,\n",
        "            decoder_output: Tensor,\n",
        "    ) -> Tensor:\n",
        "        assert encoder_output.dim() == decoder_output.dim()\n",
        "\n",
        "        if encoder_output.dim() == 3 and decoder_output.dim() == 3:  # Train\n",
        "            seq_lengths = encoder_output.size(1)\n",
        "            target_lengths = decoder_output.size(1)\n",
        "\n",
        "            encoder_output = encoder_output.unsqueeze(2)\n",
        "            decoder_output = decoder_output.unsqueeze(1)\n",
        "\n",
        "            encoder_output = encoder_output.repeat(1, 1, target_lengths, 1)\n",
        "            decoder_output = decoder_output.repeat(1, seq_lengths, 1, 1)\n",
        "\n",
        "        output = torch.cat((encoder_output, decoder_output), dim=-1)\n",
        "        output = self.fc(output).log_softmax(dim=-1)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFp2kFhcv1B9"
      },
      "source": [
        "# Hyperparameters and Model initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1B_el7DAv5eF"
      },
      "outputs": [],
      "source": [
        "class HParams():\n",
        "    \"\"\"       \n",
        "    HParams: Hyperparameters needed to initialize the Conformer model     \n",
        "    Args:\n",
        "        num_vocabs (int): The number of vocabulary\n",
        "        model_size (str, optional): Size of the model['small', 'medium', 'large'] (default : 'medium')\n",
        "        input_dim (int, optional): Dimension of input vector (default : 80)\n",
        "        encoder_num_layers (int, optional): The number of convolutional layers (default : 5)\n",
        "        decoder_num_layers (int, optional): The number of rnn layers (default : 1)\n",
        "        kernel_size (int, optional): Value of convolution kernel size (default : 5)\n",
        "        num_channels (int, optional): The number of channels in the convolution filter (default: 256)\n",
        "        hidden_dim (int, optional): The number of features in the decoder hidden state (default : 2048)\n",
        "        encoder_output_dim (int, optional): Dimension of encoder output vector (default: 640)\n",
        "        decoder_output_dim (int, optional): Dimension of decoder output vector (default: 640)\n",
        "        dropout (float, optional): Dropout probability of decoder (default: 0.3)\n",
        "        rnn_type (str, optional): Type of RNN cell (default: lstm)\n",
        "        sos_id (int, optional): Index of the start of sentence (default: 1)\n",
        "\n",
        "    \"\"\"\n",
        "    num_vocabs = 256\n",
        "    model_size = \"small\" # ['small', 'medium', 'large']\n",
        "    input_dim = 80\n",
        "    encoder_num_layers =  5\n",
        "    decoder_num_layers = 1\n",
        "    kernel_size = 5\n",
        "    num_channels = 256\n",
        "    hidden_dim = 2048\n",
        "    encoder_output_dim = 640\n",
        "    decoder_output_dim = 640\n",
        "    dropout = 0.3\n",
        "    rnn_type = \"lstm\"\n",
        "    sos_id = 1\n",
        "    preprocessing_params = {\n",
        "        \"sample_rate\": 16000,\n",
        "        \"win_length_ms\": 25,\n",
        "        \"hop_length_ms\": 10,\n",
        "        \"n_fft\": 512,\n",
        "        \"n_mels\": 80,\n",
        "        \"normalize\": False,\n",
        "        \"mean\": -5.6501,\n",
        "        \"std\": 4.2280,\n",
        "\n",
        "        \"spec_augment\": True,\n",
        "        \"mF\": 2,\n",
        "        \"F\": 27,\n",
        "        \"mT\": 5,\n",
        "        \"pS\": 0.05\n",
        "    }\n",
        "\n",
        "    training_params = {\n",
        "    # \"epochs\": 250,\n",
        "    # \"batch_size\": 16,\n",
        "    # \"accumulated_steps\": 4,\n",
        "    # \"mixed_precision\": True,\n",
        "\n",
        "    \"beta1\": 0.9,\n",
        "    \"beta2\": 0.98,\n",
        "    \"eps\": 1e-9,\n",
        "    \"weight_decay\": 1e-6,\n",
        "    \"lr\": 0.0025,\n",
        "\n",
        "    \"schedule_dim\": 144,\n",
        "    \"warmup_steps\": 10000,\n",
        "    \"K\": 2\n",
        "    }\n",
        "\n",
        "params = HParams()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOElT-pqv5zq",
        "outputId": "1bf434da-92c7-4bb2-ea61-35bee99b56e8"
      },
      "outputs": [],
      "source": [
        "model = ContextNet(params.preprocessing_params).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAOjdLqwsonx",
        "outputId": "d70ed36b-10bc-46e1-93d1-47b9ea9146f7"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(PATH + \"Contextnet.pth\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8nfAkBWOpGz"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYrzNO1f1Udm"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(params=model.parameters(), \n",
        "                             lr=params.training_params[\"lr\"], \n",
        "                             betas=(params.training_params[\"beta1\"], params.training_params[\"beta2\"]), \n",
        "                             eps=params.training_params[\"eps\"], \n",
        "                             weight_decay=params.training_params[\"weight_decay\"])\n",
        "\n",
        "scheduler = transformers.get_linear_schedule_with_warmup(optimizer=optimizer, \n",
        "                                                         num_warmup_steps=params.training_params[\"warmup_steps\"],\n",
        "                                                         num_training_steps=85, \n",
        "                                                         last_epoch = -1) \n",
        "\n",
        "# optimizer.load_state_dict(torch.load(PATH + \"Contextnet_256_optimizer.pth\"))\n",
        "# scheduler.load_state_dict(torch.load(PATH + \"Contextnet_256_scheduler.pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaNHjyVLPKfr"
      },
      "outputs": [],
      "source": [
        "scaler = torch.cuda.amp.GradScaler()\n",
        "path = PATH + \"Contextnet\"\n",
        "\n",
        "trainer = Trainer(model, optimizer, scheduler, scaler, path)\n",
        "#trainer.train(train_dataloader, dev_dataloader)\n",
        "trainer.evaluate(dev_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDiuOVcpeNzt"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnUgfLBEeNEs"
      },
      "outputs": [],
      "source": [
        "def test(test_dataset, path):\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    speech_true = []\n",
        "    speech_pred = []\n",
        "    total_wer = 0.0\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with open(path, \"w\") as f:\n",
        "      writer = csv.writer(f)\n",
        "      # Evaluation Loop\n",
        "      for step, batch in enumerate(tqdm(test_dataset)):\n",
        "\n",
        "          inputs, targets, input_len, target_len = batch\n",
        "\n",
        "          # Sequence Prediction\n",
        "          with torch.no_grad():\n",
        "\n",
        "              outputs_pred = greedy_search_decoding(inputs, input_len)\n",
        "\n",
        "          # Sequence Truth\n",
        "          outputs_true = tokenizer.decode(targets.tolist())\n",
        "\n",
        "          # Compute Batch wer and Update total wer\n",
        "          batch_wer = jiwer.wer(outputs_true, outputs_pred, standardize=True)\n",
        "          total_wer += batch_wer\n",
        "\n",
        "          # Update String lists\n",
        "          speech_true += outputs_true\n",
        "          speech_pred += outputs_pred\n",
        "\n",
        "          # Prediction Verbose\n",
        "          print(\"Groundtruths :\\n\", outputs_true)\n",
        "          print(\"Predictions :\\n\", outputs_pred)\n",
        "\n",
        "          # Eval Loss\n",
        "          with torch.no_grad():\n",
        "              pred, pred_len = model.forward(inputs, input_len, targets, target_len)\n",
        "              batch_loss = warp_rnnt.rnnt_loss(\n",
        "                                  log_probs=torch.nn.functional.log_softmax(pred, dim=-1),\n",
        "                                  labels=targets.int(),\n",
        "                                  frames_lengths=pred_len.int(),\n",
        "                                  labels_lengths=target_len.int(),\n",
        "                                  average_frames=False,\n",
        "                                  reduction='mean',\n",
        "                                  blank=0,\n",
        "                                  gather=True)\n",
        "              # batch_loss = self.loss(pred, targets, pred_len, target_len)\n",
        "              total_loss += batch_loss\n",
        "\n",
        "          # Step print\n",
        "          print(\"\\nmean batch wer {:.2f}% - batch wer: {:.2f}% - mean loss {:.4f} - batch loss: {:.4f}\".format(100 * total_wer / (step + 1), 100 * batch_wer, total_loss / (step + 1), batch_loss))\n",
        "\n",
        "          writer.writerow([outputs_true,outputs_pred,100 * batch_wer,batch_loss])\n",
        "\n",
        "    # Compute wer\n",
        "    if total_wer / test_dataset.__len__() > 1:\n",
        "        wer = 1\n",
        "    else:\n",
        "        wer = jiwer.wer(speech_true, speech_pred, standardize=True)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = total_loss / test_dataset.__len__()\n",
        "\n",
        "    return wer, speech_true, speech_pred, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aG7ay1Mg3eoC"
      },
      "outputs": [],
      "source": [
        "test_clean_dataset = LibriSpeechDataset(\"test-clean\", tokenizer)\n",
        "test_other_dataset = LibriSpeechDataset(\"test-other\", tokenizer)\n",
        "\n",
        "test_clean_dataloader = torch.utils.data.DataLoader(test_clean_dataset,\n",
        "                                                    batch_size=4,\n",
        "                                                    shuffle=False,\n",
        "                                                    collate_fn=collate_fn,\n",
        "                                                    drop_last=True)\n",
        "\n",
        "test_other_dataloader = torch.utils.data.DataLoader(test_other_dataset,\n",
        "                                                    batch_size=4,\n",
        "                                                    shuffle=False,\n",
        "                                                    collate_fn=collate_fn,\n",
        "                                                    drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k95q7YUQ9F4A"
      },
      "outputs": [],
      "source": [
        "path_test_clean = PATH + \"Contextnet_test_clean.csv\"\n",
        "wer_clean, _, _, loss_clean = test(test_clean_dataloader, path_test_clean)\n",
        "print(wer_clean, loss_clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38ndYzc-wODe"
      },
      "outputs": [],
      "source": [
        "path_test_other = PATH + \"Contextnet_test_other.csv\"\n",
        "wer_clean, _, _, loss_clean = test(test_other_dataloader, path_test_other)\n",
        "print(wer_clean,loss_clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQNKQovI7M1Q"
      },
      "outputs": [],
      "source": [
        "path2 = PATH + \"Conformer\"\n",
        "path = PATH + \"Contextnet\"\n",
        "\n",
        "fig = plt.figure()\n",
        "\n",
        "with open(path2+\".tsv\", \"r\") as f:\n",
        "    reader = csv.reader(f, delimiter=\"\\t\")\n",
        "\n",
        "    p = []\n",
        "    for i, row in enumerate(reader):\n",
        "        p.append(float(row[1]))\n",
        "\n",
        "    print(p)\n",
        "    p1, = plt.plot(p, label=\"Conformer\")\n",
        "\n",
        "with open(path+\".tsv\", \"r\") as f:\n",
        "    reader = csv.reader(f, delimiter=\"\\t\")\n",
        "    p = []\n",
        "    for row in reader:\n",
        "        p.append(float(row[1]))\n",
        "\n",
        "    print(p)\n",
        "    p2, = plt.plot(p, label=\"ContextNet\")\n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"WER\")\n",
        "plt.legend(handles=[p1,p2], loc=1, fontsize='small', fancybox=True)\n",
        "plt.show()\n",
        "fig.savefig(PATH + \"wer_plot.png\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "H2kfRbNprD5U",
        "x7ModXBbvLEj",
        "I3cgY2buvrbL",
        "X4CF8k257cL3",
        "sDp8hxwKvd8B",
        "w-42JvCe6uH7"
      ],
      "name": "Contextnet_1000_SpeechRecognition.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
